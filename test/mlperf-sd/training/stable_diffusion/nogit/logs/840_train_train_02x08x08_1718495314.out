pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:40 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
STARTING TIMING RUN AT 2024-06-15 11:59:42 PM
:::MLLOG {"namespace": "", "time_ms": 1718495987575, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987641, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987655, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987728, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987781, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987783, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987787, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495987787, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989122, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989375, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989496, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989498, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989508, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989520, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495989519, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495990138, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718495995522, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995564, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995571, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995574, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495995580, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495995557, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995582, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995586, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995592, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495995598, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495995610, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495995623, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2569886787, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495995629, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
[rank: 1] Global seed set to 2569886787
:::MLLOG {"namespace": "", "time_ms": 1718495995616, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995654, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995661, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995667, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495995635, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995678, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495995690, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995695, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
Using base config ['./configs/train_02x08x08.yaml']
:::MLLOG {"namespace": "", "time_ms": 1718495995703, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995703, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3053581307, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495995709, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495995715, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
[rank: 0] Global seed set to 3053581307
:::MLLOG {"namespace": "", "time_ms": 1718495995726, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995728, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495995733, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495995738, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495995754, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
Using ckpt_path = /checkpoints/sd/512-base-ema.ckpt
:::MLLOG {"namespace": "", "time_ms": 1718495995762, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1251502030, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495995765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
[rank: 3] Global seed set to 1251502030
:::MLLOG {"namespace": "", "time_ms": 1718495995779, "event_type": "POINT_IN_TIME", "key": "seed", "value": 426624066, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 2] Global seed set to 426624066
:::MLLOG {"namespace": "", "time_ms": 1718495995789, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995831, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995836, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995801, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995848, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495995854, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995861, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495995868, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995877, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995881, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495995887, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495995893, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495995899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495995875, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995905, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495995913, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2508094377, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495995923, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995929, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
[rank: 5] Global seed set to 2508094377
:::MLLOG {"namespace": "", "time_ms": 1718495995934, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495995917, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495995947, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495995956, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3670388381, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495995959, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495995971, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
[rank: 4] Global seed set to 3670388381
:::MLLOG {"namespace": "", "time_ms": 1718495995988, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495995994, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495996003, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495996009, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495996015, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495996021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495996027, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495996035, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3295512907, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495996039, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
[rank: 7] Global seed set to 3295512907
:::MLLOG {"namespace": "", "time_ms": 1718495996050, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495996058, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2956804431, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 6] Global seed set to 2956804431
:::MLLOG {"namespace": "", "time_ms": 1718495996487, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495996533, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495996551, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495996556, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495996560, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495996563, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495996568, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495996570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495996584, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1598707271, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 9] Global seed set to 1598707271
:::MLLOG {"namespace": "", "time_ms": 1718495997047, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997078, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997083, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997091, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997092, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997096, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997099, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997105, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3917132528, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 11] Global seed set to 3917132528
:::MLLOG {"namespace": "", "time_ms": 1718495997165, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997195, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997197, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997199, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997201, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997203, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997206, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997212, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2805125859, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 8] Global seed set to 2805125859
:::MLLOG {"namespace": "", "time_ms": 1718495997219, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997251, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997254, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
LatentDiffusion: Running in v-prediction mode
:::MLLOG {"namespace": "", "time_ms": 1718495997257, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997260, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997262, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997264, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997270, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2460263097, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 10] Global seed set to 2460263097
:::MLLOG {"namespace": "", "time_ms": 1718495997309, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997340, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997343, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997346, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997348, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997361, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997365, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997374, "event_type": "POINT_IN_TIME", "key": "seed", "value": 829625498, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 13] Global seed set to 829625498
:::MLLOG {"namespace": "", "time_ms": 1718495997425, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997456, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997459, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997462, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997466, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997469, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997471, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997473, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997477, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3776444714, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 12] Global seed set to 3776444714
:::MLLOG {"namespace": "", "time_ms": 1718495997463, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997487, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997492, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997494, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997497, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718495997500, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997483, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718495997503, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997506, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718495997508, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997510, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718495997515, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718495997515, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2395450244, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718495997517, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
[rank: 15] Global seed set to 2395450244
:::MLLOG {"namespace": "", "time_ms": 1718495997523, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718495997525, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718495997527, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718495997533, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2152484189, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 14] Global seed set to 2152484189
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072408, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072409, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072409, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072409, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496072410, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072411, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072412, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072414, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072415, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072416, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072418, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072420, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072421, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072425, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072427, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072428, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072429, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496072430, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496072433, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting learning rate to 1.60e-05 = 1 (accumulate_grad_batches) * 16 (num_gpus) * 8 (local_batch_size) * 1.25e-07 (base_lr)
:::MLLOG {"namespace": "", "time_ms": 1718496072434, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 3] Global seed set to 1251502030
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/16
[rank: 2] Global seed set to 426624066
[rank: 1] Global seed set to 2569886787
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/16
[rank: 0] Global seed set to 3053581307
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/16
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072499, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496072501, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072503, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072505, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072505, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496072507, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496072509, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072513, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496072515, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
[rank: 7] Global seed set to 3295512907
:::MLLOG {"namespace": "", "time_ms": 1718496072516, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072518, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496072518, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496072520, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072521, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496072522, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 6] Global seed set to 2956804431
:::MLLOG {"namespace": "", "time_ms": 1718496072523, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/16
:::MLLOG {"namespace": "", "time_ms": 1718496072525, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072526, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496072528, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496072529, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 5] Global seed set to 2508094377
[rank: 4] Global seed set to 3670388381
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/16
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073422, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496073425, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073425, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496073426, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496073427, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496073429, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496073430, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073430, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073430, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496073431, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073432, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073433, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073433, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496073434, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496073436, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073440, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718496073443, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073446, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496073449, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
[rank: 13] Global seed set to 829625498
:::MLLOG {"namespace": "", "time_ms": 1718496073451, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/16
:::MLLOG {"namespace": "", "time_ms": 1718496073459, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496073462, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718496073468, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496073470, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718496073472, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496073474, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
[rank: 12] Global seed set to 3776444714
:::MLLOG {"namespace": "", "time_ms": 1718496073475, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/16
:::MLLOG {"namespace": "", "time_ms": 1718496073485, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496073486, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496073487, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073488, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073490, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718496073491, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073492, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718496073497, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 8] Global seed set to 2805125859
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718496073502, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
[rank: 10] Global seed set to 2460263097
:::MLLOG {"namespace": "", "time_ms": 1718496073506, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/16
:::MLLOG {"namespace": "", "time_ms": 1718496073508, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 14] Global seed set to 2152484189
[rank: 15] Global seed set to 2395450244
:::MLLOG {"namespace": "", "time_ms": 1718496073509, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/16
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/16
:::MLLOG {"namespace": "", "time_ms": 1718496073511, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 11] Global seed set to 3917132528
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/16
[rank: 9] Global seed set to 1598707271
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/16
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 16 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-56_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-56_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-55_train_02x08x08/diff_tb
worker-1:13229:13229 [0] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13229:13229 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13229:13229 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13229:13229 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13229:13229 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13229:13229 [0] NCCL INFO cudaDriverVersion 12030
NCCL version 2.19.3+cuda12.3
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-56_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T23-59-57_train_02x08x08/diff_tb
worker-1:13229:13314 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13229:13314 [0] NCCL INFO P2P plugin IBext
worker-1:13229:13314 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13229:13314 [0] NCCL INFO Using non-device net plugin version 0
worker-1:13229:13314 [0] NCCL INFO Using network IBext
worker-1:13229:13314 [0] NCCL INFO comm 0x56074b7e78c0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13229:13314 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13229:13314 [0] NCCL INFO Setting affinity for GPU 0 to 01
worker-1:13229:13314 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-1:13229:13314 [0] NCCL INFO NVLS Head  0:  0  8
worker-1:13229:13314 [0] NCCL INFO NVLS Head  1:  1  9
worker-1:13229:13314 [0] NCCL INFO NVLS Head  2:  2 10
worker-1:13229:13314 [0] NCCL INFO NVLS Head  3:  3 11
worker-1:13229:13314 [0] NCCL INFO NVLS Head  4:  4 12
worker-1:13229:13314 [0] NCCL INFO NVLS Head  5:  5 13
worker-1:13229:13314 [0] NCCL INFO NVLS Head  6:  6 14
worker-1:13229:13314 [0] NCCL INFO NVLS Head  7:  7 15
worker-1:13229:13314 [0] NCCL INFO Channel 00/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-1:13229:13314 [0] NCCL INFO Channel 01/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-1:13229:13314 [0] NCCL INFO Channel 02/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 03/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 04/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 05/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 06/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 07/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 08/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-1:13229:13314 [0] NCCL INFO Channel 09/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-1:13229:13314 [0] NCCL INFO Channel 10/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 11/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 12/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 13/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 14/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Channel 15/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-1:13229:13314 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] -1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->7 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7 [8] 1/-1/-1->0->8 [9] -1/-1/-1->0->7 [10] 1/-1/-1->0->7 [11] 1/-1/-1->0->7 [12] 1/-1/-1->0->7 [13] 1/-1/-1->0->7 [14] 1/-1/-1->0->7 [15] 1/-1/-1->0->7
worker-1:13229:13314 [0] NCCL INFO P2P Chunksize set to 131072
worker-1:13229:13314 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:13172:13172 [2] NCCL INFO cudaDriverVersion 12030
worker-0:13172:13172 [2] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13172:13172 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13172:13172 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13172:13172 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13172:13172 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13172:13253 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13172:13253 [2] NCCL INFO P2P plugin IBext
worker-0:13172:13253 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13172:13253 [2] NCCL INFO Using non-device net plugin version 0
worker-0:13172:13253 [2] NCCL INFO Using network IBext
worker-0:13172:13253 [2] NCCL INFO comm 0x5643ecf8f0c0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13172:13253 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13172:13253 [2] NCCL INFO Setting affinity for GPU 2 to 04
worker-0:13172:13253 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-0:13172:13253 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->2 [3] -1/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9 [8] 11/-1/-1->10->9 [9] 11/-1/-1->10->9 [10] 11/2/-1->10->-1 [11] -1/-1/-1->10->9 [12] 11/-1/-1->10->9 [13] 11/-1/-1->10->9 [14] 11/-1/-1->10->9 [15] 11/-1/-1->10->9
worker-0:13172:13253 [2] NCCL INFO P2P Chunksize set to 131072
worker-0:13172:13253 [2] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13228 [2] NCCL INFO cudaDriverVersion 12030
worker-1:13228:13228 [2] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13228:13228 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13228:13228 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13228:13228 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13228:13228 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13228:13315 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13228:13315 [2] NCCL INFO P2P plugin IBext
worker-1:13228:13315 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13228:13315 [2] NCCL INFO Using non-device net plugin version 0
worker-1:13228:13315 [2] NCCL INFO Using network IBext
worker-0:13172:13253 [2] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 08/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 10/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO comm 0x55eefffbb310 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13228:13315 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13228:13315 [2] NCCL INFO Setting affinity for GPU 2 to 04
worker-1:13228:13315 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-1:13228:13315 [2] NCCL INFO NVLS Head  0:  0  8
worker-1:13228:13315 [2] NCCL INFO NVLS Head  1:  1  9
worker-1:13228:13315 [2] NCCL INFO NVLS Head  2:  2 10
worker-1:13228:13315 [2] NCCL INFO NVLS Head  3:  3 11
worker-1:13228:13315 [2] NCCL INFO NVLS Head  4:  4 12
worker-1:13228:13315 [2] NCCL INFO NVLS Head  5:  5 13
worker-1:13228:13315 [2] NCCL INFO NVLS Head  6:  6 14
worker-1:13228:13315 [2] NCCL INFO NVLS Head  7:  7 15
worker-0:13172:13253 [2] NCCL INFO Channel 11/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 12/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 13/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 14/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 15/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Connected all rings
worker-0:13172:13253 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/10/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->10 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
worker-1:13228:13315 [2] NCCL INFO P2P Chunksize set to 131072
worker-1:13228:13315 [2] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 08/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 09/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Connected all rings
worker-1:13228:13315 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:13169:13169 [3] NCCL INFO cudaDriverVersion 12030
worker-0:13169:13169 [3] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13169:13169 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13169:13169 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13169:13169 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13169:13169 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13169:13255 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13169:13255 [3] NCCL INFO P2P plugin IBext
worker-0:13169:13255 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13169:13255 [3] NCCL INFO Using non-device net plugin version 0
worker-0:13169:13255 [3] NCCL INFO Using network IBext
worker-1:13228:13315 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO comm 0x55e87715bf30 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13169:13255 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13169:13255 [3] NCCL INFO Setting affinity for GPU 3 to 08
worker-0:13169:13255 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-0:13169:13255 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->3 [4] -1/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] 12/-1/-1->11->10 [7] 12/-1/-1->11->10 [8] 12/-1/-1->11->10 [9] 12/-1/-1->11->10 [10] 12/-1/-1->11->10 [11] 12/3/-1->11->-1 [12] -1/-1/-1->11->10 [13] 12/-1/-1->11->10 [14] 12/-1/-1->11->10 [15] 12/-1/-1->11->10
worker-0:13169:13255 [3] NCCL INFO P2P Chunksize set to 131072
worker-0:13169:13255 [3] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13227 [3] NCCL INFO cudaDriverVersion 12030
worker-1:13227:13227 [3] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13227:13227 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13227:13227 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13227:13227 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13227:13227 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13227:13320 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13227:13320 [3] NCCL INFO P2P plugin IBext
worker-1:13227:13320 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13227:13320 [3] NCCL INFO Using non-device net plugin version 0
worker-1:13227:13320 [3] NCCL INFO Using network IBext
worker-0:13169:13255 [3] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 08/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 09/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO comm 0x55fe3595d5f0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13227:13320 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13227:13320 [3] NCCL INFO Setting affinity for GPU 3 to 08
worker-1:13227:13320 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-1:13227:13320 [3] NCCL INFO NVLS Head  0:  0  8
worker-1:13227:13320 [3] NCCL INFO NVLS Head  1:  1  9
worker-1:13227:13320 [3] NCCL INFO NVLS Head  2:  2 10
worker-1:13227:13320 [3] NCCL INFO NVLS Head  3:  3 11
worker-1:13227:13320 [3] NCCL INFO NVLS Head  4:  4 12
worker-1:13227:13320 [3] NCCL INFO NVLS Head  5:  5 13
worker-1:13227:13320 [3] NCCL INFO NVLS Head  6:  6 14
worker-1:13227:13320 [3] NCCL INFO NVLS Head  7:  7 15
worker-0:13169:13255 [3] NCCL INFO Channel 11/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 12/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 13/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 14/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 15/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Connected all rings
worker-0:13169:13255 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 06/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/11/-1->3->-1 [4] -1/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->11 [12] -1/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
worker-1:13227:13320 [3] NCCL INFO P2P Chunksize set to 131072
worker-1:13227:13320 [3] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 08/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Connected all rings
worker-1:13227:13320 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:13181:13181 [0] NCCL INFO cudaDriverVersion 12030
worker-0:13181:13181 [0] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13181:13181 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13181:13181 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13181:13181 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13181:13181 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13181:13254 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13181:13254 [0] NCCL INFO P2P plugin IBext
worker-0:13181:13254 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13181:13254 [0] NCCL INFO Using non-device net plugin version 0
worker-0:13181:13254 [0] NCCL INFO Using network IBext
worker-1:13227:13320 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO comm 0x564291159740 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13181:13254 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13181:13254 [0] NCCL INFO Setting affinity for GPU 0 to 01
worker-0:13181:13254 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-0:13181:13254 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] -1/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/-1/-1->8->15 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15 [8] 9/0/-1->8->-1 [9] -1/-1/-1->8->15 [10] 9/-1/-1->8->15 [11] 9/-1/-1->8->15 [12] 9/-1/-1->8->15 [13] 9/-1/-1->8->15 [14] 9/-1/-1->8->15 [15] 9/-1/-1->8->15
worker-0:13181:13254 [0] NCCL INFO P2P Chunksize set to 131072
worker-0:13181:13254 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13231:13231 [5] NCCL INFO cudaDriverVersion 12030
worker-1:13231:13231 [5] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13231:13231 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13231:13231 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13231:13231 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13231:13231 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13231:13318 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13231:13318 [5] NCCL INFO P2P plugin IBext
worker-1:13231:13318 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13231:13318 [5] NCCL INFO Using non-device net plugin version 0
worker-1:13231:13318 [5] NCCL INFO Using network IBext
worker-0:13181:13254 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 08/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 09/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 10/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 11/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 12/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 13/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO comm 0x5604af9c8220 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13231:13318 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13231:13318 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-1:13231:13318 [5] NCCL INFO NVLS Head  0:  0  8
worker-1:13231:13318 [5] NCCL INFO NVLS Head  1:  1  9
worker-1:13231:13318 [5] NCCL INFO NVLS Head  2:  2 10
worker-1:13231:13318 [5] NCCL INFO NVLS Head  3:  3 11
worker-1:13231:13318 [5] NCCL INFO NVLS Head  4:  4 12
worker-1:13231:13318 [5] NCCL INFO NVLS Head  5:  5 13
worker-1:13231:13318 [5] NCCL INFO NVLS Head  6:  6 14
worker-1:13231:13318 [5] NCCL INFO NVLS Head  7:  7 15
worker-0:13181:13254 [0] NCCL INFO Channel 14/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Connected all rings
worker-0:13181:13254 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 08/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 10/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/13/-1->5->-1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->13 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4
worker-1:13231:13318 [5] NCCL INFO P2P Chunksize set to 131072
worker-1:13231:13318 [5] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:13173:13173 [4] NCCL INFO cudaDriverVersion 12030
worker-0:13173:13173 [4] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13173:13173 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13173:13173 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13173:13173 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13173:13173 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13173:13259 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13173:13259 [4] NCCL INFO P2P plugin IBext
worker-0:13173:13259 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13173:13259 [4] NCCL INFO Using non-device net plugin version 0
worker-0:13173:13259 [4] NCCL INFO Using network IBext
worker-1:13231:13318 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Connected all rings
worker-1:13231:13318 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO comm 0x564929e31110 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13173:13259 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13173:13259 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-0:13173:13259 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->4 [5] -1/-1/-1->12->11 [6] 13/-1/-1->12->11 [7] 13/-1/-1->12->11 [8] 13/-1/-1->12->11 [9] 13/-1/-1->12->11 [10] 13/-1/-1->12->11 [11] 13/-1/-1->12->11 [12] 13/4/-1->12->-1 [13] -1/-1/-1->12->11 [14] 13/-1/-1->12->11 [15] 13/-1/-1->12->11
worker-0:13173:13259 [4] NCCL INFO P2P Chunksize set to 131072
worker-0:13173:13259 [4] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 08/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 09/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 10/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:13226:13226 [1] NCCL INFO cudaDriverVersion 12030
worker-1:13226:13226 [1] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13226:13226 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13226:13226 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13226:13226 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13226:13226 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13226:13319 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13226:13319 [1] NCCL INFO P2P plugin IBext
worker-1:13226:13319 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13226:13319 [1] NCCL INFO Using non-device net plugin version 0
worker-1:13226:13319 [1] NCCL INFO Using network IBext
worker-0:13173:13259 [4] NCCL INFO Channel 12/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 13/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 14/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 15/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Connected all rings
worker-0:13173:13259 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 08/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO comm 0x5568b3fff0f0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13226:13319 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13226:13319 [1] NCCL INFO Setting affinity for GPU 1 to 02
worker-1:13226:13319 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-1:13226:13319 [1] NCCL INFO NVLS Head  0:  0  8
worker-1:13226:13319 [1] NCCL INFO NVLS Head  1:  1  9
worker-1:13226:13319 [1] NCCL INFO NVLS Head  2:  2 10
worker-1:13226:13319 [1] NCCL INFO NVLS Head  3:  3 11
worker-1:13226:13319 [1] NCCL INFO NVLS Head  4:  4 12
worker-1:13226:13319 [1] NCCL INFO NVLS Head  5:  5 13
worker-1:13226:13319 [1] NCCL INFO NVLS Head  6:  6 14
worker-1:13226:13319 [1] NCCL INFO NVLS Head  7:  7 15
worker-0:13173:13259 [4] NCCL INFO Channel 09/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/9/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->9 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
worker-1:13226:13319 [1] NCCL INFO P2P Chunksize set to 131072
worker-1:13226:13319 [1] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:13171:13171 [6] NCCL INFO cudaDriverVersion 12030
worker-0:13171:13171 [6] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13171:13171 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13171:13171 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13171:13171 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13171:13171 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13171:13257 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13171:13257 [6] NCCL INFO P2P plugin IBext
worker-0:13171:13257 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13171:13257 [6] NCCL INFO Using non-device net plugin version 0
worker-0:13171:13257 [6] NCCL INFO Using network IBext
worker-1:13226:13319 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Connected all rings
worker-1:13226:13319 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO comm 0x55fcce7f57a0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13171:13257 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13171:13257 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-0:13171:13257 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->6 [7] -1/-1/-1->14->13 [8] 15/-1/-1->14->13 [9] 15/-1/-1->14->13 [10] 15/-1/-1->14->13 [11] 15/-1/-1->14->13 [12] 15/-1/-1->14->13 [13] 15/-1/-1->14->13 [14] 15/6/-1->14->-1 [15] -1/-1/-1->14->13
worker-0:13171:13257 [6] NCCL INFO P2P Chunksize set to 131072
worker-0:13171:13257 [6] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 03/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 06/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 07/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 08/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 09/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 10/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:13234:13234 [4] NCCL INFO cudaDriverVersion 12030
worker-1:13234:13234 [4] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13234:13234 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13234:13234 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13234:13234 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13234:13234 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13234:13317 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13234:13317 [4] NCCL INFO P2P plugin IBext
worker-1:13234:13317 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13234:13317 [4] NCCL INFO Using non-device net plugin version 0
worker-1:13234:13317 [4] NCCL INFO Using network IBext
worker-0:13171:13257 [6] NCCL INFO Channel 11/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 12/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 14/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 15/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Connected all rings
worker-0:13171:13257 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 08/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO comm 0x55d18aacac70 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13234:13317 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13234:13317 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-1:13234:13317 [4] NCCL INFO NVLS Head  0:  0  8
worker-1:13234:13317 [4] NCCL INFO NVLS Head  1:  1  9
worker-1:13234:13317 [4] NCCL INFO NVLS Head  2:  2 10
worker-1:13234:13317 [4] NCCL INFO NVLS Head  3:  3 11
worker-1:13234:13317 [4] NCCL INFO NVLS Head  4:  4 12
worker-1:13234:13317 [4] NCCL INFO NVLS Head  5:  5 13
worker-1:13234:13317 [4] NCCL INFO NVLS Head  6:  6 14
worker-1:13234:13317 [4] NCCL INFO NVLS Head  7:  7 15
worker-0:13171:13257 [6] NCCL INFO Channel 09/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/12/-1->4->-1 [5] -1/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->12 [13] -1/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
worker-1:13234:13317 [4] NCCL INFO P2P Chunksize set to 131072
worker-1:13234:13317 [4] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:13167:13167 [1] NCCL INFO cudaDriverVersion 12030
worker-0:13167:13167 [1] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13167:13167 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13167:13167 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13167:13167 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13167:13167 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13167:13258 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13167:13258 [1] NCCL INFO P2P plugin IBext
worker-0:13167:13258 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13167:13258 [1] NCCL INFO Using non-device net plugin version 0
worker-0:13167:13258 [1] NCCL INFO Using network IBext
worker-1:13234:13317 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Connected all rings
worker-1:13234:13317 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO comm 0x55ed8c8e4cc0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13167:13258 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13167:13258 [1] NCCL INFO Setting affinity for GPU 1 to 02
worker-0:13167:13258 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-0:13167:13258 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->1 [2] -1/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8 [8] 10/-1/-1->9->8 [9] 10/1/-1->9->-1 [10] -1/-1/-1->9->8 [11] 10/-1/-1->9->8 [12] 10/-1/-1->9->8 [13] 10/-1/-1->9->8 [14] 10/-1/-1->9->8 [15] 10/-1/-1->9->8
worker-0:13167:13258 [1] NCCL INFO P2P Chunksize set to 131072
worker-0:13167:13258 [1] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 09/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 10/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 11/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:13232:13232 [6] NCCL INFO cudaDriverVersion 12030
worker-1:13232:13232 [6] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13232:13232 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13232:13232 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13232:13232 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13232:13232 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13232:13321 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13232:13321 [6] NCCL INFO P2P plugin IBext
worker-1:13232:13321 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13232:13321 [6] NCCL INFO Using non-device net plugin version 0
worker-1:13232:13321 [6] NCCL INFO Using network IBext
worker-0:13167:13258 [1] NCCL INFO Channel 12/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 13/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 14/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 15/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Connected all rings
worker-0:13167:13258 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 08/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO comm 0x558acde0c860 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13232:13321 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13232:13321 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-1:13232:13321 [6] NCCL INFO NVLS Head  0:  0  8
worker-1:13232:13321 [6] NCCL INFO NVLS Head  1:  1  9
worker-1:13232:13321 [6] NCCL INFO NVLS Head  2:  2 10
worker-1:13232:13321 [6] NCCL INFO NVLS Head  3:  3 11
worker-1:13232:13321 [6] NCCL INFO NVLS Head  4:  4 12
worker-1:13232:13321 [6] NCCL INFO NVLS Head  5:  5 13
worker-1:13232:13321 [6] NCCL INFO NVLS Head  6:  6 14
worker-1:13232:13321 [6] NCCL INFO NVLS Head  7:  7 15
worker-0:13167:13258 [1] NCCL INFO Channel 09/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/14/-1->6->-1 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->14 [15] -1/-1/-1->6->5
worker-1:13232:13321 [6] NCCL INFO P2P Chunksize set to 131072
worker-1:13232:13321 [6] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:13170:13170 [5] NCCL INFO cudaDriverVersion 12030
worker-0:13170:13170 [5] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13170:13170 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13170:13170 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13170:13170 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13170:13170 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13170:13256 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13170:13256 [5] NCCL INFO P2P plugin IBext
worker-0:13170:13256 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13170:13256 [5] NCCL INFO Using non-device net plugin version 0
worker-0:13170:13256 [5] NCCL INFO Using network IBext
worker-1:13232:13321 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Connected all rings
worker-1:13232:13321 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO comm 0x556ff89de0c0 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13170:13256 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13170:13256 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-0:13170:13256 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->5 [6] -1/-1/-1->13->12 [7] 14/-1/-1->13->12 [8] 14/-1/-1->13->12 [9] 14/-1/-1->13->12 [10] 14/-1/-1->13->12 [11] 14/-1/-1->13->12 [12] 14/-1/-1->13->12 [13] 14/5/-1->13->-1 [14] -1/-1/-1->13->12 [15] 14/-1/-1->13->12
worker-0:13170:13256 [5] NCCL INFO P2P Chunksize set to 131072
worker-0:13170:13256 [5] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 05/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 08/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 09/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 10/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:13233:13233 [7] NCCL INFO cudaDriverVersion 12030
worker-1:13233:13233 [7] NCCL INFO Bootstrap : Using eth0:10.113.5.242<0>
worker-1:13233:13233 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:13233:13233 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:13233:13233 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:13233:13233 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:13233:13316 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:13233:13316 [7] NCCL INFO P2P plugin IBext
worker-1:13233:13316 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.242<0>
worker-1:13233:13316 [7] NCCL INFO Using non-device net plugin version 0
worker-1:13233:13316 [7] NCCL INFO Using network IBext
worker-0:13170:13256 [5] NCCL INFO Channel 11/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 13/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 14/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 15/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Connected all rings
worker-0:13170:13256 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 03/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 07/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 08/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO comm 0x55c3b0b8a8f0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x58a7f25772fb6ba4 - Init START
worker-1:13233:13316 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:13233:13316 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-1:13233:13316 [7] NCCL INFO NVLS Head  0:  0  8
worker-1:13233:13316 [7] NCCL INFO NVLS Head  1:  1  9
worker-1:13233:13316 [7] NCCL INFO NVLS Head  2:  2 10
worker-1:13233:13316 [7] NCCL INFO NVLS Head  3:  3 11
worker-1:13233:13316 [7] NCCL INFO NVLS Head  4:  4 12
worker-1:13233:13316 [7] NCCL INFO NVLS Head  5:  5 13
worker-1:13233:13316 [7] NCCL INFO NVLS Head  6:  6 14
worker-1:13233:13316 [7] NCCL INFO NVLS Head  7:  7 15
worker-0:13170:13256 [5] NCCL INFO Channel 09/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] 0/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/15/-1->7->-1 [8] -1/-1/-1->7->6 [9] 0/-1/-1->7->6 [10] 0/-1/-1->7->6 [11] 0/-1/-1->7->6 [12] 0/-1/-1->7->6 [13] 0/-1/-1->7->6 [14] 0/-1/-1->7->6 [15] 0/-1/-1->7->15
worker-1:13233:13316 [7] NCCL INFO P2P Chunksize set to 131072
worker-1:13233:13316 [7] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:13174:13174 [7] NCCL INFO cudaDriverVersion 12030
worker-0:13174:13174 [7] NCCL INFO Bootstrap : Using eth0:10.113.4.37<0>
worker-0:13174:13174 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:13174:13174 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:13174:13174 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:13174:13174 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:13174:13260 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:13174:13260 [7] NCCL INFO P2P plugin IBext
worker-0:13174:13260 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.37<0>
worker-0:13174:13260 [7] NCCL INFO Using non-device net plugin version 0
worker-0:13174:13260 [7] NCCL INFO Using network IBext
worker-1:13233:13316 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Connected all rings
worker-1:13233:13316 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO comm 0x5572362dae70 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x58a7f25772fb6ba4 - Init START
worker-0:13174:13260 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:13174:13260 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-0:13174:13260 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] 8/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->7 [8] -1/-1/-1->15->14 [9] 8/-1/-1->15->14 [10] 8/-1/-1->15->14 [11] 8/-1/-1->15->14 [12] 8/-1/-1->15->14 [13] 8/-1/-1->15->14 [14] 8/-1/-1->15->14 [15] 8/7/-1->15->-1
worker-0:13174:13260 [7] NCCL INFO P2P Chunksize set to 131072
worker-0:13174:13260 [7] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 07/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 08/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 09/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 10/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 11/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 12/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 13/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 15/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Connected all rings
worker-0:13174:13260 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 04/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 08/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 11/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 12/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 13/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 14/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Connected all rings
worker-1:13229:13314 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:13229:13314 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Connected all trees
worker-1:13229:13314 [0] NCCL INFO NVLS comm 0x56074b7e78c0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13229:13314 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:13228:13315 [2] NCCL INFO Connected all trees
worker-1:13228:13315 [2] NCCL INFO NVLS comm 0x55eefffbb310 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13228:13315 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:13231:13318 [5] NCCL INFO Connected all trees
worker-1:13231:13318 [5] NCCL INFO NVLS comm 0x5604af9c8220 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13231:13318 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Channel 14/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13227:13320 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13227:13320 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:13227:13320 [3] NCCL INFO Connected all trees
worker-1:13227:13320 [3] NCCL INFO NVLS comm 0x55fe3595d5f0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13227:13320 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13227:13320 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13227:13320 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13227:13320 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13233:13316 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13233:13316 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:13233:13316 [7] NCCL INFO Connected all trees
worker-1:13233:13316 [7] NCCL INFO NVLS comm 0x55c3b0b8a8f0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13233:13316 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13169:13255 [3] NCCL INFO Channel 09/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 10/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 11/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 13/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 14/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 15/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:13169:13255 [3] NCCL INFO Channel 10/0 : 11[3] -> 10[2] via P2P/CUMEM
Configure sharded model for LatentDiffusion
worker-1:13233:13316 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Connected all trees
worker-0:13169:13255 [3] NCCL INFO NVLS comm 0x55e87715bf30 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13169:13255 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:13169:13255 [3] NCCL INFO Connected NVLS tree
worker-1:13226:13319 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13169:13255 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13226:13319 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:13226:13319 [1] NCCL INFO Connected all trees
worker-1:13226:13319 [1] NCCL INFO NVLS comm 0x5568b3fff0f0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13226:13319 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 09/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 10/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 11/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 12/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 13/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 14/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 15/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Channel 14/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:13174:13260 [7] NCCL INFO Connected all trees
worker-1:13226:13319 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13174:13260 [7] NCCL INFO NVLS comm 0x5572362dae70 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13174:13260 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13226:13319 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13174:13260 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13226:13319 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13174:13260 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:13174:13260 [7] NCCL INFO Connected NVLS tree
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13234:13317 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13174:13260 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13174:13260 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13234:13317 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:13234:13317 [4] NCCL INFO Connected all trees
worker-1:13234:13317 [4] NCCL INFO NVLS comm 0x55d18aacac70 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13234:13317 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 11/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 12/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 13/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 14/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 15/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 15/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:13181:13254 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Connected all trees
worker-1:13234:13317 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13181:13254 [0] NCCL INFO NVLS comm 0x564291159740 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13181:13254 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 14/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:13181:13254 [0] NCCL INFO Connected NVLS tree
worker-1:13232:13321 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13181:13254 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13181:13254 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13232:13321 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:13232:13321 [6] NCCL INFO Connected all trees
worker-1:13232:13321 [6] NCCL INFO NVLS comm 0x558acde0c860 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:13232:13321 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 11/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 12/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 13/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 14/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 15/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Channel 08/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:13167:13258 [1] NCCL INFO Connected all trees
worker-1:13232:13321 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13167:13258 [1] NCCL INFO NVLS comm 0x55ed8c8e4cc0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13167:13258 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:13167:13258 [1] NCCL INFO Connected NVLS tree
worker-1:13234:13317 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-0:13167:13258 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13167:13258 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13232:13321 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 10/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 12/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 13/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 14/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 15/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Channel 09/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:13172:13253 [2] NCCL INFO Connected all trees
worker-0:13172:13253 [2] NCCL INFO NVLS comm 0x5643ecf8f0c0 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13172:13253 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:13172:13253 [2] NCCL INFO Connected NVLS tree
worker-0:13172:13253 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13172:13253 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:13173:13259 [4] NCCL INFO Channel 10/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 11/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 12/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 14/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 15/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Channel 11/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:13173:13259 [4] NCCL INFO Connected all trees
worker-0:13173:13259 [4] NCCL INFO NVLS comm 0x564929e31110 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13173:13259 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:13173:13259 [4] NCCL INFO Connected NVLS tree
worker-0:13173:13259 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13173:13259 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:13170:13256 [5] NCCL INFO Channel 10/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 11/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 12/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 13/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 15/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Channel 12/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:13170:13256 [5] NCCL INFO Connected all trees
worker-0:13170:13256 [5] NCCL INFO NVLS comm 0x556ff89de0c0 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13170:13256 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 14/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:13170:13256 [5] NCCL INFO Connected NVLS tree
worker-0:13170:13256 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13170:13256 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:13171:13257 [6] NCCL INFO Channel 10/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 11/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 12/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 13/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 14/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Channel 13/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:13171:13257 [6] NCCL INFO Connected all trees
worker-0:13171:13257 [6] NCCL INFO NVLS comm 0x55fcce7f57a0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:13171:13257 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:13171:13257 [6] NCCL INFO Connected NVLS tree
worker-0:13171:13257 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:13171:13257 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
Deleting key model.diffusion_model.out.0.bias from state_dict.
Deleting key model.diffusion_model.out.2.weight from state_dict.
Deleting key model.diffusion_model.out.2.bias from state_dict.
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
Missing Keys:
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
Deleting key model.diffusion_model.out.0.bias from state_dict.
Deleting key model.diffusion_model.out.2.weight from state_dict.
Deleting key model.diffusion_model.out.2.bias from state_dict.
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
Missing Keys:
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
worker-1:13229:13314 [0] NCCL INFO Channel 14/0 : 8[0] ->Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
:::MLLOG {"namespace": "", "time_ms": 1718496290867, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1639}}
:::MLLOG {"namespace": "", "time_ms": 1718496290987, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1718496290990, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1641}}
:::MLLOG {"namespace": "", "time_ms": 1718496290993, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1642}}
:::MLLOG {"namespace": "", "time_ms": 1718496290994, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1643}}
:::MLLOG {"namespace": "", "time_ms": 1718496290996, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.6e-05, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1644}}
:::MLLOG {"namespace": "", "time_ms": 1718496290999, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1650}}
Setting up LambdaLR scheduler...
Project config
model:
  base_learning_rate: 1.25e-07
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    parameterization: v
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: npy
    first_stage_type: moments
    cond_stage_key: txt
    image_size: 64
    channels: 4
    cond_stage_trainable: false
    conditioning_key: crossattn
    monitor: steps
    scale_factor: 0.18215
    use_ema: false
    load_vae: true
    load_unet: false
    load_encoder: true
    validation_config:
      sampler: ddim
      steps: 50
      scale: 8.0
      ddim_eta: 0.0
      prompt_key: caption
      image_fname_key: image_id
      save_images:
        enabled: false
        base_output_dir: /results/inference
      fid:
        enabled: true
        inception_weights_url: https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth
        cache_dir: /checkpoints/inception
        gt_path: /datasets/coco2014/val2014_30k_stats.npz
      clip:
        enabled: true
        clip_version: ViT-H-14
        cache_dir: /checkpoints/clip
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 1000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        use_checkpoint: false
        use_fp16: true
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        num_head_channels: 64
        use_spatial_transformer: true
        use_linear_in_transformer: true
        transformer_depth: 1
        context_dim: 1024
        legacy: false
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder
      params:
        arch: ViT-H-14
        version: laion2b_s32b_b79k
        freeze: true
        layer: penultimate
        cache_dir: /checkpoints/clip
    use_fp16: true
    ckpt: /checkpoints/sd/512-base-ema.ckpt
data:
  target: ldm.data.composable_data_module.ComposableDataModule
  params:
    train:
      target: ldm.data.webdatasets.build_dataloader
      params:
        urls: /datasets/laion-400m/webdataset-moments-filtered/{00000..00831}.tar
        batch_size: 8
        shuffle: 1000
        partial: false
        keep_only_keys:
        - npy
        - txt
        num_workers: 4
        persistent_workers: true
    validation:
      target: ldm.data.tsv.build_dataloader
      params:
        annotations_file: /datasets/coco2014/val2014_30k.tsv
        keys:
        - image_id
        - id
        - caption
        batch_size: 8
        shuffle: false
        num_workers: 1

Lightning config
trainer:
  accelerator: gpu
  num_nodes: 2
  devices: 8
  precision: 16
  logger: false
  log_every_n_steps: 5
  enable_progress_bar: false
  max_epochs: -1
  max_steps: 10000000000000
  val_check_interval: 1000
  enable_checkpointing: true
  num_sanity_val_steps: 0
  strategy:
    target: strategies.DDPStrategy
    params:
      find_unused_parameters: false
modelcheckpoint:
  target: lightning.pytorch.callbacks.ModelCheckpoint
  params:
    save_top_k: -1
    every_n_train_steps: 1000

:::MLLOG {"namespace": "", "time_ms": 1718496291071, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 88}}
SLURM auto-requeueing enabled. Setting signal handlers.

  | Name              | Type                   | Params
-------------------------------------------------------------
0 | model             | DiffusionWrapper       | 865 M 
1 | first_stage_model | AutoencoderKL          | 83.7 M
2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
-------------------------------------------------------------
865 M     Trainable params
437 M     Non-trainable params
1.3 B     Total params
2,607.194 Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.
  rank_zero_warn(
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
wSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:13173:13259 [4] NCCL INFO comm 0x564929e31110 rank 1Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
orker-1:13228:13315 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:13228:13315 [2] NCCL INFO Connected NVLS tree
worker-1:13228:13315 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13228:13315 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13228:13315 [2] NCCL INFO comm 0x55eefffbb310 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:13234:13317 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:13181:13254 [0] NCCL INFO comm 0x564291159740 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-1:13231:13318 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
commId 0x58a7f25772fb6ba4 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:13172:13253 [2] Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
nd] via NET/IBext/1/GDRDMA
worker-1:13231:13318 [5] NCCL INFO Connected NVLS tree
worker-1:13231:13318 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13231:13318 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13231:13318 [5] NCCL INFO comm 0x5604af9c8220 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
nd] via NET/IBext/0/GDRDMA
worker-1:13234:13317 [4] NCCL INFO Connected NVLS tree
worker-1:13234:13317 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13234:13317 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13234:13317 [4] NCCL INFO comm 0x55d18aacac70 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 160 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
worker-1:13233:13316 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [sendSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
NCCL INFO comm 0x5643ecf8f0c0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:13169:13255 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels peSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-1:13227:13320 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMASetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Training is starting
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:13174:13260 [7] NCCL INFO comm 0x5572362dae70 rank 15 nranks 16 cudaDev 7 nvmlDeSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.

worker-1:13227:13320 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:13227:13320 [3] NCCL INFO Connected NVLS tree
worker-1:13227:13320 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13227:13320 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13227:13320 [3] NCCL INFO comm 0x55fe3595d5f0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
] via NET/IBext/3/GDRDMA
worker-1:13233:13316 [7] NCCL INFO Connected NVLS tree
worker-1:13233:13316 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13233:13316 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13233:13316 [7] NCCL INFO comm 0x55c3b0b8a8f0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
v 7 busId b7000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
r peer
worker-0:13169:13255 [3] NCCL INFO comm 0x55e87715bf30 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
worker-0:13171:13257 [6] NCCL INFO comm 0x55fcce7f57a0 rank 1Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-1:13226:13319 [1] NCCL INFOSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
4 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
 Channel 15/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:13226:13319 [1] NCCL INFO Connected NVLS tree
worker-1:13226:13319 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13226:13319 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13226:13319 [1] NCCL INFO comm 0x5568b3fff0f0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
worker-1:13232:13321 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [seSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:13167:13258 [1] NCCL INFO comm 0x55ed8c8e4cc0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
nd] via NET/IBext/2/GDRDMA
worker-1:13232:13321 [6] NCCL INFO Connected NVLS tree
worker-1:13232:13321 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13232:13321 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13232:13321 [6] NCCL INFO comm 0x558acde0c860 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
Id 0x58a7f25772fb6ba4 - Init COMPLETE
worker-0:13170:13256 [5] NCCL INFO comm 0x556ff89de0c0 rank 1Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-1:13229:13314 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:13229:13314 [0] NCCL INFO Connected NVLS tree
worker-1:13229:13314 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:13229:13314 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:13229:13314 [0] NCCL INFO comm 0x56074b7e78c0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
3 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x58a7f25772fb6ba4 - Init COMPLETE
:::MLLOG {"namespace": "", "time_ms": 1718496294506, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 0}}
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
:::MLLOG {"namespace": "", "time_ms": 1718496403985, "event_type": "POINT_IN_TIME", "key": "loss", "value": 1.059883713722229, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718496403997, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.5840144159999998e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718496403998, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718496404095, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718496511234, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.7970527410507202, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718496511239, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 3.1840128159999994e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718496511241, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718496511293, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718496617298, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.7541561126708984, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1718496617302, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 4.784011215999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1718496617303, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1718496617343, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 300}}
:::MLLOG {"namespace": "", "time_ms": 1718496725210, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.6357159614562988, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1718496725215, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 6.384009615999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1718496725218, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1718496725255, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 400}}
:::MLLOG {"namespace": "", "time_ms": 1718496833070, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4003355801105499, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1718496833074, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 7.984008015999999e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1718496833076, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1718496833120, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 500}}
:::MLLOG {"namespace": "", "time_ms": 1718496918645, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.49256670475006104, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1718496918650, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 9.584006416e-06, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1718496918652, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1718496918685, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 600}}
:::MLLOG {"namespace": "", "time_ms": 1718496962295, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.6151587963104248, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1718496962298, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.1184004815999999e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1718496962300, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1718496962333, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 700}}
:::MLLOG {"namespace": "", "time_ms": 1718497005706, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4565173387527466, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1718497005710, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.2784003215999998e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1718497005712, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1718497005748, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 800}}
:::MLLOG {"namespace": "", "time_ms": 1718497050785, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.567070484161377, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1718497050790, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.4384001615999999e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1718497050802, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1718497050844, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 900}}
:::MLLOG {"namespace": "", "time_ms": 1718497098658, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.367281436920166, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1718497098670, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.5984000016e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1718497098671, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1718497262936, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 119}}
:::MLLOG {"namespace": "", "time_ms": 1718497263856, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718497311779, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718497348211, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718497352244, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718497388536, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718497392569, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718497428835, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718497432867, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718497469161, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718497473201, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718497509477, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718497513508, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718497549822, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718497553856, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718497590139, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718497594181, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718497630479, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718497634513, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718497670803, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718497674833, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718497711172, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718497715215, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718497751542, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718497755582, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718497791901, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718497795938, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718497832272, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718497836311, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718497872645, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718497876672, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718497912964, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718497916998, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718497953339, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718497957364, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718497993673, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718497997708, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718498034031, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718498038063, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718498074407, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718498078444, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718498114775, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718498118822, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718498155146, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718498159185, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718498195531, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718498199561, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718498235887, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718498239918, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 230}}
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('validation/fid', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('validation/clip', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
:::MLLOG {"namespace": "", "time_ms": 1718498281841, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 432.0194153748077, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 124, "step_num": 1000, "metric": "FID"}}
:::MLLOG {"namespace": "", "time_ms": 1718498281845, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.040313720703125, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 128, "step_num": 1000, "metric": "CLIP"}}
:::MLLOG {"namespace": "", "time_ms": 1718498281847, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 131}}
:::MLLOG {"namespace": "", "time_ms": 1718498281870, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1000}}
:::MLLOG {"namespace": "", "time_ms": 1718498327033, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4004131853580475, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1718498327037, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1718498327039, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1718498327068, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1100}}
:::MLLOG {"namespace": "", "time_ms": 1718498372492, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.423064261674881, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1718498372495, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1718498372497, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1718498372524, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1200}}
:::MLLOG {"namespace": "", "time_ms": 1718498417450, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.44959384202957153, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1718498417454, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1718498417456, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1718498417493, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1300}}
:::MLLOG {"namespace": "", "time_ms": 1718498462215, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.440153032541275, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1718498462219, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1718498462221, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1718498462255, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1400}}
:::MLLOG {"namespace": "", "time_ms": 1718498506076, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.44090890884399414, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1718498506080, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1718498506083, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1718498506114, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1500}}
:::MLLOG {"namespace": "", "time_ms": 1718498547331, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.35017839074134827, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1718498547335, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1718498547338, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1718498547373, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1600}}
:::MLLOG {"namespace": "", "time_ms": 1718498592264, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.36051350831985474, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1718498592267, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1718498592269, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1718498592303, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1700}}
:::MLLOG {"namespace": "", "time_ms": 1718498636305, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3566280007362366, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1718498636309, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1718498636311, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1718498636346, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1800}}
:::MLLOG {"namespace": "", "time_ms": 1718498681930, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.32046347856521606, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1718498681933, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1718498681935, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1718498681967, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 1900}}
:::MLLOG {"namespace": "", "time_ms": 1718498727101, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3122158944606781, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1718498727111, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1718498727114, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1718498938611, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 119}}
:::MLLOG {"namespace": "", "time_ms": 1718498938882, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718498943008, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718498979541, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718498983598, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718499020127, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718499024181, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718499060726, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718499064771, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718499101318, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718499105363, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718499141938, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718499145995, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718499182542, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718499186594, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718499223145, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718499227203, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718499263718, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718499267779, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718499304320, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718499308365, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718499344901, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718499348955, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718499385524, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718499389577, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718499426125, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718499430175, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718499466735, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718499470792, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718499507340, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718499511386, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718499547908, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718499551968, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718499588541, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718499592605, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718499629214, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718499633284, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718499669845, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718499673901, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718499710424, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718499714480, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718499751024, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718499755084, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718499791604, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718499795649, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718499832218, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718499836272, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718499872833, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718499876882, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718499921946, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 252.91852177639996, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 124, "step_num": 2000, "metric": "FID"}}
:::MLLOG {"namespace": "", "time_ms": 1718499921950, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.04571533203125, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 128, "step_num": 2000, "metric": "CLIP"}}
:::MLLOG {"namespace": "", "time_ms": 1718499921952, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 131}}
:::MLLOG {"namespace": "", "time_ms": 1718499921968, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2000}}
:::MLLOG {"namespace": "", "time_ms": 1718499967563, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3592686653137207, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2100}}
:::MLLOG {"namespace": "", "time_ms": 1718499967568, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2100}}
:::MLLOG {"namespace": "", "time_ms": 1718499967569, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2100}}
:::MLLOG {"namespace": "", "time_ms": 1718499967605, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2100}}
:::MLLOG {"namespace": "", "time_ms": 1718500011665, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3937058746814728, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2200}}
:::MLLOG {"namespace": "", "time_ms": 1718500011670, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2200}}
:::MLLOG {"namespace": "", "time_ms": 1718500011673, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2200}}
:::MLLOG {"namespace": "", "time_ms": 1718500011708, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2200}}
:::MLLOG {"namespace": "", "time_ms": 1718500057762, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3313644826412201, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2300}}
:::MLLOG {"namespace": "", "time_ms": 1718500057767, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2300}}
:::MLLOG {"namespace": "", "time_ms": 1718500057768, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2300}}
:::MLLOG {"namespace": "", "time_ms": 1718500057802, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2300}}
:::MLLOG {"namespace": "", "time_ms": 1718500102630, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.35691213607788086, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2400}}
:::MLLOG {"namespace": "", "time_ms": 1718500102633, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2400}}
:::MLLOG {"namespace": "", "time_ms": 1718500102635, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2400}}
:::MLLOG {"namespace": "", "time_ms": 1718500102675, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2400}}
:::MLLOG {"namespace": "", "time_ms": 1718500148533, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3914446532726288, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2500}}
:::MLLOG {"namespace": "", "time_ms": 1718500148537, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2500}}
:::MLLOG {"namespace": "", "time_ms": 1718500148539, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2500}}
:::MLLOG {"namespace": "", "time_ms": 1718500148572, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2500}}
:::MLLOG {"namespace": "", "time_ms": 1718500193117, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.37815219163894653, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2600}}
:::MLLOG {"namespace": "", "time_ms": 1718500193120, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2600}}
:::MLLOG {"namespace": "", "time_ms": 1718500193122, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2600}}
:::MLLOG {"namespace": "", "time_ms": 1718500193154, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2600}}
:::MLLOG {"namespace": "", "time_ms": 1718500238702, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.2983090877532959, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2700}}
:::MLLOG {"namespace": "", "time_ms": 1718500238707, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2700}}
:::MLLOG {"namespace": "", "time_ms": 1718500238709, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2700}}
:::MLLOG {"namespace": "", "time_ms": 1718500238742, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2700}}
:::MLLOG {"namespace": "", "time_ms": 1718500283926, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4019184410572052, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2800}}
:::MLLOG {"namespace": "", "time_ms": 1718500283930, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2800}}
:::MLLOG {"namespace": "", "time_ms": 1718500283932, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2800}}
:::MLLOG {"namespace": "", "time_ms": 1718500283963, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2800}}
:::MLLOG {"namespace": "", "time_ms": 1718500328462, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3265156149864197, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 2900}}
:::MLLOG {"namespace": "", "time_ms": 1718500328467, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 2900}}
:::MLLOG {"namespace": "", "time_ms": 1718500328469, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 2900}}
:::MLLOG {"namespace": "", "time_ms": 1718500328503, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 2900}}
:::MLLOG {"namespace": "", "time_ms": 1718500374442, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3752460479736328, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3000}}
:::MLLOG {"namespace": "", "time_ms": 1718500374453, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3000}}
:::MLLOG {"namespace": "", "time_ms": 1718500374455, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3000}}
:::MLLOG {"namespace": "", "time_ms": 1718500592215, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 119}}
:::MLLOG {"namespace": "", "time_ms": 1718500592471, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718500596609, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718500633165, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718500637215, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718500673723, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718500677769, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718500714348, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718500718407, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718500754944, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718500758992, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718500795512, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718500799566, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718500836085, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718500840144, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718500876672, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718500880729, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718500917224, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718500921283, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718500957865, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718500961916, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718500998482, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718501002523, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718501039107, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718501043153, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718501079749, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718501083812, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718501120401, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718501124459, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718501161045, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718501165108, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718501201653, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718501205701, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718501242235, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718501246281, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718501282856, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718501286899, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718501323458, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718501327527, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718501364089, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718501368137, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718501404716, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718501408777, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718501445340, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718501449397, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718501485958, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718501490023, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718501526580, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718501530642, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718501574522, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 214.07259755069134, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 124, "step_num": 3000, "metric": "FID"}}
:::MLLOG {"namespace": "", "time_ms": 1718501574526, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.07861328125, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 128, "step_num": 3000, "metric": "CLIP"}}
:::MLLOG {"namespace": "", "time_ms": 1718501574528, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 131}}
:::MLLOG {"namespace": "", "time_ms": 1718501574555, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3000}}
:::MLLOG {"namespace": "", "time_ms": 1718501618736, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.32369062304496765, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3100}}
:::MLLOG {"namespace": "", "time_ms": 1718501618740, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3100}}
:::MLLOG {"namespace": "", "time_ms": 1718501618741, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3100}}
:::MLLOG {"namespace": "", "time_ms": 1718501618777, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3100}}
:::MLLOG {"namespace": "", "time_ms": 1718501664010, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4481000304222107, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3200}}
:::MLLOG {"namespace": "", "time_ms": 1718501664015, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3200}}
:::MLLOG {"namespace": "", "time_ms": 1718501664018, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3200}}
:::MLLOG {"namespace": "", "time_ms": 1718501664049, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3200}}
:::MLLOG {"namespace": "", "time_ms": 1718501712170, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3403130769729614, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3300}}
:::MLLOG {"namespace": "", "time_ms": 1718501712174, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3300}}
:::MLLOG {"namespace": "", "time_ms": 1718501712175, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3300}}
:::MLLOG {"namespace": "", "time_ms": 1718501712201, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3300}}
:::MLLOG {"namespace": "", "time_ms": 1718501760056, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.2819029688835144, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3400}}
:::MLLOG {"namespace": "", "time_ms": 1718501760062, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3400}}
:::MLLOG {"namespace": "", "time_ms": 1718501760064, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3400}}
:::MLLOG {"namespace": "", "time_ms": 1718501760104, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3400}}
:::MLLOG {"namespace": "", "time_ms": 1718501813990, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.4260876178741455, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3500}}
:::MLLOG {"namespace": "", "time_ms": 1718501813994, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3500}}
:::MLLOG {"namespace": "", "time_ms": 1718501813996, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3500}}
:::MLLOG {"namespace": "", "time_ms": 1718501814033, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3500}}
:::MLLOG {"namespace": "", "time_ms": 1718501857937, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.31917983293533325, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3600}}
:::MLLOG {"namespace": "", "time_ms": 1718501857942, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3600}}
:::MLLOG {"namespace": "", "time_ms": 1718501857943, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3600}}
:::MLLOG {"namespace": "", "time_ms": 1718501857973, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3600}}
:::MLLOG {"namespace": "", "time_ms": 1718501901560, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3090924024581909, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3700}}
:::MLLOG {"namespace": "", "time_ms": 1718501901564, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3700}}
:::MLLOG {"namespace": "", "time_ms": 1718501901566, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3700}}
:::MLLOG {"namespace": "", "time_ms": 1718501901601, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3700}}
:::MLLOG {"namespace": "", "time_ms": 1718501947186, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3540126085281372, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3800}}
:::MLLOG {"namespace": "", "time_ms": 1718501947193, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3800}}
:::MLLOG {"namespace": "", "time_ms": 1718501947205, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3800}}
:::MLLOG {"namespace": "", "time_ms": 1718501947234, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3800}}
:::MLLOG {"namespace": "", "time_ms": 1718501990599, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.31758618354797363, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 3900}}
:::MLLOG {"namespace": "", "time_ms": 1718501990602, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 3900}}
:::MLLOG {"namespace": "", "time_ms": 1718501990604, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 3900}}
:::MLLOG {"namespace": "", "time_ms": 1718501990632, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 3900}}
:::MLLOG {"namespace": "", "time_ms": 1718502037915, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.357889860868454, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4000}}
:::MLLOG {"namespace": "", "time_ms": 1718502037925, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4000}}
:::MLLOG {"namespace": "", "time_ms": 1718502037927, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4000}}
:::MLLOG {"namespace": "", "time_ms": 1718502251647, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 119}}
:::MLLOG {"namespace": "", "time_ms": 1718502251894, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718502256029, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 0}}
:::MLLOG {"namespace": "", "time_ms": 1718502292608, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718502296664, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1718502333231, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718502337288, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 20}}
:::MLLOG {"namespace": "", "time_ms": 1718502373768, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718502377823, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 30}}
:::MLLOG {"namespace": "", "time_ms": 1718502414358, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718502418418, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 40}}
:::MLLOG {"namespace": "", "time_ms": 1718502454962, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718502459004, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 50}}
:::MLLOG {"namespace": "", "time_ms": 1718502495491, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718502499531, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 60}}
:::MLLOG {"namespace": "", "time_ms": 1718502536041, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718502540095, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 70}}
:::MLLOG {"namespace": "", "time_ms": 1718502576641, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718502580686, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 80}}
:::MLLOG {"namespace": "", "time_ms": 1718502617213, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718502621268, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 90}}
:::MLLOG {"namespace": "", "time_ms": 1718502657817, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718502661884, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 100}}
:::MLLOG {"namespace": "", "time_ms": 1718502698457, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718502702490, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 110}}
:::MLLOG {"namespace": "", "time_ms": 1718502739055, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718502743114, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 120}}
:::MLLOG {"namespace": "", "time_ms": 1718502779660, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718502783722, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 130}}
:::MLLOG {"namespace": "", "time_ms": 1718502820235, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718502824273, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 140}}
:::MLLOG {"namespace": "", "time_ms": 1718502860832, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718502864882, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 150}}
:::MLLOG {"namespace": "", "time_ms": 1718502901482, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718502905535, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 160}}
:::MLLOG {"namespace": "", "time_ms": 1718502942092, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718502946137, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 170}}
:::MLLOG {"namespace": "", "time_ms": 1718502982700, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718502986761, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 180}}
:::MLLOG {"namespace": "", "time_ms": 1718503023311, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718503027370, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 190}}
:::MLLOG {"namespace": "", "time_ms": 1718503063877, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718503067923, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 200}}
:::MLLOG {"namespace": "", "time_ms": 1718503104487, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718503108544, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 210}}
:::MLLOG {"namespace": "", "time_ms": 1718503145127, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718503149176, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 220}}
:::MLLOG {"namespace": "", "time_ms": 1718503185698, "event_type": "INTERVAL_START", "key": "block_start", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 142, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718503189753, "event_type": "INTERVAL_END", "key": "block_stop", "value": "validation_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 147, "step_num": 230}}
:::MLLOG {"namespace": "", "time_ms": 1718503233728, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 201.46890085078897, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 124, "step_num": 4000, "metric": "FID"}}
:::MLLOG {"namespace": "", "time_ms": 1718503233732, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.08038330078125, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 128, "step_num": 4000, "metric": "CLIP"}}
:::MLLOG {"namespace": "", "time_ms": 1718503233733, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4000, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 131}}
:::MLLOG {"namespace": "", "time_ms": 1718503233760, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4000}}
:::MLLOG {"namespace": "", "time_ms": 1718503278266, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3619815707206726, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4100}}
:::MLLOG {"namespace": "", "time_ms": 1718503278270, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4100}}
:::MLLOG {"namespace": "", "time_ms": 1718503278271, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4100}}
:::MLLOG {"namespace": "", "time_ms": 1718503278310, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4100}}
:::MLLOG {"namespace": "", "time_ms": 1718503324004, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.39729636907577515, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4200}}
:::MLLOG {"namespace": "", "time_ms": 1718503324008, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4200}}
:::MLLOG {"namespace": "", "time_ms": 1718503324010, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4200}}
:::MLLOG {"namespace": "", "time_ms": 1718503324042, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4200}}
:::MLLOG {"namespace": "", "time_ms": 1718503370103, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.2791162133216858, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4300}}
:::MLLOG {"namespace": "", "time_ms": 1718503370108, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4300}}
:::MLLOG {"namespace": "", "time_ms": 1718503370110, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4300}}
:::MLLOG {"namespace": "", "time_ms": 1718503370145, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4300}}
:::MLLOG {"namespace": "", "time_ms": 1718503414662, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.32763671875, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4400}}
:::MLLOG {"namespace": "", "time_ms": 1718503414667, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4400}}
:::MLLOG {"namespace": "", "time_ms": 1718503414669, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4400}}
:::MLLOG {"namespace": "", "time_ms": 1718503414705, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4400}}
:::MLLOG {"namespace": "", "time_ms": 1718503457633, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.31907379627227783, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4500}}
:::MLLOG {"namespace": "", "time_ms": 1718503457637, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4500}}
:::MLLOG {"namespace": "", "time_ms": 1718503457638, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4500}}
:::MLLOG {"namespace": "", "time_ms": 1718503457674, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4500}}
:::MLLOG {"namespace": "", "time_ms": 1718503504779, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.34034863114356995, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4600}}
:::MLLOG {"namespace": "", "time_ms": 1718503504783, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4600}}
:::MLLOG {"namespace": "", "time_ms": 1718503504784, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4600}}
:::MLLOG {"namespace": "", "time_ms": 1718503504814, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4600}}
:::MLLOG {"namespace": "", "time_ms": 1718503549640, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3398914635181427, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4700}}
:::MLLOG {"namespace": "", "time_ms": 1718503549643, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4700}}
:::MLLOG {"namespace": "", "time_ms": 1718503549645, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4700}}
:::MLLOG {"namespace": "", "time_ms": 1718503549676, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4700}}
:::MLLOG {"namespace": "", "time_ms": 1718503594077, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.3718932271003723, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4800}}
:::MLLOG {"namespace": "", "time_ms": 1718503594081, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4800}}
:::MLLOG {"namespace": "", "time_ms": 1718503594083, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4800}}
:::MLLOG {"namespace": "", "time_ms": 1718503594114, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4800}}
:::MLLOG {"namespace": "", "time_ms": 1718503640371, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.32696405053138733, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 4900}}
:::MLLOG {"namespace": "", "time_ms": 1718503640375, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 4900}}
:::MLLOG {"namespace": "", "time_ms": 1718503640377, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 4900}}
:::MLLOG {"namespace": "", "time_ms": 1718503640415, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 4900}}
:::MLLOG {"namespace": "", "time_ms": 1718503683592, "event_type": "POINT_IN_TIME", "key": "loss", "value": 0.33258676528930664, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 114, "step_num": 5000}}
:::MLLOG {"namespace": "", "time_ms": 1718503683603, "event_type": "POINT_IN_TIME", "key": "lr_abs", "value": 1.6e-05, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 115, "step_num": 5000}}
:::MLLOG {"namespace": "", "time_ms": 1718503683605, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 116, "step_num": 5000}}
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: got SIGCONT
slurmstepd: error: *** JOB 840 ON worker-1 CANCELLED AT 2024-06-16T02:11:00 ***
srun: forcing job termination
slurmstepd: error: *** STEP 840.0 ON worker-1 CANCELLED AT 2024-06-16T02:11:00 ***
bypassing sigterm
srun: error: Timed out waiting for job step to complete
