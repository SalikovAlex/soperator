pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
STARTING TIMING RUN AT 2024-06-15 07:35:25 PM
:::MLLOG {"namespace": "", "time_ms": 1718480131938, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132073, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132260, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132547, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132640, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132754, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480132761, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480137381, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480139040, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480139066, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480139069, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480139070, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480139071, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480139073, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480139074, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480139075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480139080, "event_type": "POINT_IN_TIME", "key": "seed", "value": 88763182, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 3] Global seed set to 88763182
:::MLLOG {"namespace": "", "time_ms": 1718480139280, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480139307, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480139309, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480139310, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480139311, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480139312, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480139314, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480139315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480139319, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1705030003, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 1] Global seed set to 1705030003
:::MLLOG {"namespace": "", "time_ms": 1718480139621, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480139657, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480139660, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480139662, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480139663, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480139665, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480139666, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480139668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480139672, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2275038997, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 6] Global seed set to 2275038997
:::MLLOG {"namespace": "", "time_ms": 1718480139793, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480139820, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480139823, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480139824, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480139825, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480139827, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480139828, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480139830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480139837, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2655080881, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 2] Global seed set to 2655080881
:::MLLOG {"namespace": "", "time_ms": 1718480139963, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480139998, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480140001, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480140004, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480140005, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480140007, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480140009, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480140010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480140016, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3686785565, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 7] Global seed set to 3686785565
:::MLLOG {"namespace": "", "time_ms": 1718480140067, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480140102, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480140104, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480140106, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480140107, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480140108, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480140109, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480140110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480140115, "event_type": "POINT_IN_TIME", "key": "seed", "value": 210431592, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 5] Global seed set to 210431592
:::MLLOG {"namespace": "", "time_ms": 1718480140170, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480140193, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480140199, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480140203, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480140205, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480140207, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480140209, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480140211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480140215, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4150270077, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 4] Global seed set to 4150270077
:::MLLOG {"namespace": "", "time_ms": 1718480150606, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480150632, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480150634, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480150636, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480150637, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480150639, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480150640, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480150641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
Using base config ['./configs/train_02x08x08.yaml']
:::MLLOG {"namespace": "", "time_ms": 1718480150645, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3740236102, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 0] Global seed set to 3740236102
Using ckpt_path = /checkpoints/sd/512-base-ema.ckpt
LatentDiffusion: Running in v-prediction mode
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
pyxis: imported docker image: cr.ai.nebius.cloud#examples/stable_diffusion_h100
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203068, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480203072, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203074, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480203076, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203077, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 1] Global seed set to 1705030003
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203257, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203257, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480203262, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203268, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203274, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480203275, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480203279, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203291, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203297, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718480203303, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203303, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203303, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480203321, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
[rank: 2] Global seed set to 2655080881
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/16
:::MLLOG {"namespace": "", "time_ms": 1718480203326, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203332, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480203338, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 7] Global seed set to 3686785565
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/16
:::MLLOG {"namespace": "", "time_ms": 1718480203344, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203350, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203350, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480203350, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480203351, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718480203355, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718480203361, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203364, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480203380, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480203381, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 4] Global seed set to 4150270077
[rank: 5] Global seed set to 210431592
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/16
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/16
:::MLLOG {"namespace": "", "time_ms": 1718480203387, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203392, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480203398, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718480203404, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 3] Global seed set to 88763182
[rank: 6] Global seed set to 2275038997
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/16
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/16
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480204393, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480204394, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480204396, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480204397, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Setting learning rate to 1.60e-05 = 1 (accumulate_grad_batches) * 16 (num_gpus) * 8 (local_batch_size) * 1.25e-07 (base_lr)
:::MLLOG {"namespace": "", "time_ms": 1718480204398, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 0] Global seed set to 3740236102
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/16
Missing logger folder: /results/2024-06-15T19-35-39_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-39_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-39_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-39_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-40_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-35-40_train_02x08x08/diff_tb
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
STARTING TIMING RUN AT 2024-06-15 07:37:38 PM
:::MLLOG {"namespace": "", "time_ms": 1718480264732, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480264809, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480264858, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480264893, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480264939, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480265011, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480265032, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480265222, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1718480272291, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272318, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272320, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272321, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272326, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272335, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272336, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272342, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3183801367, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 8] Global seed set to 3183801367
:::MLLOG {"namespace": "", "time_ms": 1718480272461, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272486, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272488, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272489, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272491, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272492, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272476, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272494, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272495, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272497, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272502, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272504, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272506, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272506, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4069852654, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718480272507, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
[rank: 14] Global seed set to 4069852654
:::MLLOG {"namespace": "", "time_ms": 1718480272509, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272510, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272514, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3290789869, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 9] Global seed set to 3290789869
:::MLLOG {"namespace": "", "time_ms": 1718480272640, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272667, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272669, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272672, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272673, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272674, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272675, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272654, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272654, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272681, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272680, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272681, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1814914261, "metadata": {"file": "main.py", "lineno": 450}}
:::MLLOG {"namespace": "", "time_ms": 1718480272682, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272683, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
[rank: 15] Global seed set to 1814914261
:::MLLOG {"namespace": "", "time_ms": 1718480272685, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272686, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272688, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272689, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272690, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272691, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272694, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272695, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272704, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1622348911, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 13] Global seed set to 1622348911
:::MLLOG {"namespace": "", "time_ms": 1718480272708, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1995060900, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 12] Global seed set to 1995060900
:::MLLOG {"namespace": "", "time_ms": 1718480272693, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272718, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272720, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272722, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272724, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272725, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272727, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272733, "event_type": "POINT_IN_TIME", "key": "seed", "value": 571138623, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 10] Global seed set to 571138623
:::MLLOG {"namespace": "", "time_ms": 1718480272880, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 72}}
:::MLLOG {"namespace": "", "time_ms": 1718480272910, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 73}}
:::MLLOG {"namespace": "", "time_ms": 1718480272913, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "reference_implementation", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 74}}
:::MLLOG {"namespace": "", "time_ms": 1718480272914, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "DGX-A100", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 75}}
:::MLLOG {"namespace": "", "time_ms": 1718480272916, "event_type": "POINT_IN_TIME", "key": "submission_poc_name", "value": "Ahmad Kiswani", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 76}}
:::MLLOG {"namespace": "", "time_ms": 1718480272918, "event_type": "POINT_IN_TIME", "key": "submission_poc_email", "value": "akiswani@nvidia.com", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 77}}
:::MLLOG {"namespace": "", "time_ms": 1718480272923, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1718480272924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "main.py", "lineno": 383}}
:::MLLOG {"namespace": "", "time_ms": 1718480272928, "event_type": "POINT_IN_TIME", "key": "seed", "value": 956455887, "metadata": {"file": "main.py", "lineno": 450}}
[rank: 11] Global seed set to 956455887
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342517, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342519, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342521, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342522, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342523, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 11] Global seed set to 956455887
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342645, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342648, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342649, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342650, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342653, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 8] Global seed set to 3183801367
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342673, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342675, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342676, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342677, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342678, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342678, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342680, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342686, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
[rank: 12] Global seed set to 1995060900
:::MLLOG {"namespace": "", "time_ms": 1718480342688, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/16
:::MLLOG {"namespace": "", "time_ms": 1718480342690, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 13] Global seed set to 1622348911
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342698, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/16
:::MLLOG {"namespace": "", "time_ms": 1718480342699, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342700, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342702, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342706, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 10] Global seed set to 571138623
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342722, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342726, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342727, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342728, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342729, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 9] Global seed set to 3290789869
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/16
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342760, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 16 processes
----------------------------------------------------------------------------------------------------

Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Missing logger folder: /results/2024-06-15T19-35-50_train_02x08x08/diff_tb
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
:::MLLOG {"namespace": "", "time_ms": 1718480342760, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 1, "metadata": {"file": "main.py", "lineno": 613}}
:::MLLOG {"namespace": "", "time_ms": 1718480342762, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342763, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "main.py", "lineno": 614}}
:::MLLOG {"namespace": "", "time_ms": 1718480342766, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342767, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "main.py", "lineno": 622}}
:::MLLOG {"namespace": "", "time_ms": 1718480342772, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342775, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 128, "metadata": {"file": "main.py", "lineno": 633}}
:::MLLOG {"namespace": "", "time_ms": 1718480342777, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
:::MLLOG {"namespace": "", "time_ms": 1718480342778, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "main.py", "lineno": 666}}
[rank: 15] Global seed set to 1814914261
[rank: 14] Global seed set to 4069852654
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/16
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/16
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
worker-1:5881:5881 [0] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5881:5881 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5881:5881 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5881:5881 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5881:5881 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5881:5881 [0] NCCL INFO cudaDriverVersion 12030
NCCL version 2.19.3+cuda12.3
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
Missing logger folder: /results/2024-06-15T19-37-52_train_02x08x08/diff_tb
worker-1:5881:6002 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5881:6002 [0] NCCL INFO P2P plugin IBext
worker-1:5881:6002 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5881:6002 [0] NCCL INFO Using non-device net plugin version 0
worker-1:5881:6002 [0] NCCL INFO Using network IBext
worker-1:5881:6002 [0] NCCL INFO comm 0x55e6f6d3f030 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x906175c8434fff47 - Init START
worker-1:5881:6002 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5881:6002 [0] NCCL INFO Setting affinity for GPU 0 to 01
worker-1:5881:6002 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-1:5881:6002 [0] NCCL INFO NVLS Head  0:  0  8
worker-1:5881:6002 [0] NCCL INFO NVLS Head  1:  1  9
worker-1:5881:6002 [0] NCCL INFO NVLS Head  2:  2 10
worker-1:5881:6002 [0] NCCL INFO NVLS Head  3:  3 11
worker-1:5881:6002 [0] NCCL INFO NVLS Head  4:  4 12
worker-1:5881:6002 [0] NCCL INFO NVLS Head  5:  5 13
worker-1:5881:6002 [0] NCCL INFO NVLS Head  6:  6 14
worker-1:5881:6002 [0] NCCL INFO NVLS Head  7:  7 15
worker-1:5881:6002 [0] NCCL INFO Channel 00/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-1:5881:6002 [0] NCCL INFO Channel 01/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-1:5881:6002 [0] NCCL INFO Channel 02/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 03/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 04/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 05/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 06/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 07/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 08/16 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
worker-1:5881:6002 [0] NCCL INFO Channel 09/16 :    0   7   6   5   4   3   2   9   8  15  14  13  12  11  10   1
worker-1:5881:6002 [0] NCCL INFO Channel 10/16 :    0   7   6   5   4   3  10   9   8  15  14  13  12  11   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 11/16 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 12/16 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 13/16 :    0   7   6  13  12  11  10   9   8  15  14   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 14/16 :    0   7  14  13  12  11  10   9   8  15   6   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Channel 15/16 :    0  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1
worker-1:5881:6002 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] -1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->7 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7 [8] 1/-1/-1->0->8 [9] -1/-1/-1->0->7 [10] 1/-1/-1->0->7 [11] 1/-1/-1->0->7 [12] 1/-1/-1->0->7 [13] 1/-1/-1->0->7 [14] 1/-1/-1->0->7 [15] 1/-1/-1->0->7
worker-1:5881:6002 [0] NCCL INFO P2P Chunksize set to 131072
worker-1:5881:6002 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-0:5959:5959 [1] NCCL INFO cudaDriverVersion 12030
worker-0:5959:5959 [1] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5959:5959 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5959:5959 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5959:5959 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5959:5959 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5959:6034 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5959:6034 [1] NCCL INFO P2P plugin IBext
worker-0:5959:6034 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5959:6034 [1] NCCL INFO Using non-device net plugin version 0
worker-0:5959:6034 [1] NCCL INFO Using network IBext
worker-0:5959:6034 [1] NCCL INFO comm 0x55cf8569d7c0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x906175c8434fff47 - Init START
worker-0:5959:6034 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5959:6034 [1] NCCL INFO Setting affinity for GPU 1 to 02
worker-0:5959:6034 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-0:5959:6034 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->1 [2] -1/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8 [8] 10/-1/-1->9->8 [9] 10/1/-1->9->-1 [10] -1/-1/-1->9->8 [11] 10/-1/-1->9->8 [12] 10/-1/-1->9->8 [13] 10/-1/-1->9->8 [14] 10/-1/-1->9->8 [15] 10/-1/-1->9->8
worker-0:5959:6034 [1] NCCL INFO P2P Chunksize set to 131072
worker-0:5959:6034 [1] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 08/0 : 9[1] -> 0[0] [send] via NET/IBext/4(8)/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 09/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 10/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 11/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 12/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-1:5873:5873 [2] NCCL INFO cudaDriverVersion 12030
worker-1:5873:5873 [2] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5873:5873 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5873:5873 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5873:5873 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5873:5873 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5873:6005 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5873:6005 [2] NCCL INFO P2P plugin IBext
worker-1:5873:6005 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5873:6005 [2] NCCL INFO Using non-device net plugin version 0
worker-1:5873:6005 [2] NCCL INFO Using network IBext
worker-0:5959:6034 [1] NCCL INFO Channel 13/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 14/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 15/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Connected all rings
worker-0:5959:6034 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 08/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 09/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO comm 0x55a42bc47700 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x906175c8434fff47 - Init START
worker-1:5873:6005 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5873:6005 [2] NCCL INFO Setting affinity for GPU 2 to 04
worker-1:5873:6005 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-1:5873:6005 [2] NCCL INFO NVLS Head  0:  0  8
worker-1:5873:6005 [2] NCCL INFO NVLS Head  1:  1  9
worker-1:5873:6005 [2] NCCL INFO NVLS Head  2:  2 10
worker-1:5873:6005 [2] NCCL INFO NVLS Head  3:  3 11
worker-1:5873:6005 [2] NCCL INFO NVLS Head  4:  4 12
worker-1:5873:6005 [2] NCCL INFO NVLS Head  5:  5 13
worker-1:5873:6005 [2] NCCL INFO NVLS Head  6:  6 14
worker-1:5873:6005 [2] NCCL INFO NVLS Head  7:  7 15
worker-0:5959:6034 [1] NCCL INFO Channel 11/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/10/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->10 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
worker-1:5873:6005 [2] NCCL INFO P2P Chunksize set to 131072
worker-1:5873:6005 [2] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [send] via NET/IBext/5(1)/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-0:5958:5958 [2] NCCL INFO cudaDriverVersion 12030
worker-0:5958:5958 [2] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5958:5958 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5958:5958 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5958:5958 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5958:5958 [2] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5958:6037 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5958:6037 [2] NCCL INFO P2P plugin IBext
worker-0:5958:6037 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5958:6037 [2] NCCL INFO Using non-device net plugin version 0
worker-0:5958:6037 [2] NCCL INFO Using network IBext
worker-1:5873:6005 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Connected all rings
worker-1:5873:6005 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO comm 0x55ecc7cdcfd0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x906175c8434fff47 - Init START
worker-0:5958:6037 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5958:6037 [2] NCCL INFO Setting affinity for GPU 2 to 04
worker-0:5958:6037 [2] NCCL INFO NVLS multicast support is available on dev 2
worker-0:5958:6037 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->2 [3] -1/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9 [8] 11/-1/-1->10->9 [9] 11/-1/-1->10->9 [10] 11/2/-1->10->-1 [11] -1/-1/-1->10->9 [12] 11/-1/-1->10->9 [13] 11/-1/-1->10->9 [14] 11/-1/-1->10->9 [15] 11/-1/-1->10->9
worker-0:5958:6037 [2] NCCL INFO P2P Chunksize set to 131072
worker-0:5958:6037 [2] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [send] via NET/IBext/5(9)/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 02/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 08/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 10/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 11/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-1:5868:5868 [1] NCCL INFO cudaDriverVersion 12030
worker-1:5868:5868 [1] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5868:5868 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5868:5868 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5868:5868 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5868:5868 [1] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5868:6003 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5868:6003 [1] NCCL INFO P2P plugin IBext
worker-1:5868:6003 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5868:6003 [1] NCCL INFO Using non-device net plugin version 0
worker-1:5868:6003 [1] NCCL INFO Using network IBext
worker-0:5958:6037 [2] NCCL INFO Channel 12/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 13/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 14/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 15/0 : 10[2] -> 9[1] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Connected all rings
worker-0:5958:6037 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 08/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO comm 0x55b68d4b1ed0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x906175c8434fff47 - Init START
worker-1:5868:6003 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5868:6003 [1] NCCL INFO Setting affinity for GPU 1 to 02
worker-1:5868:6003 [1] NCCL INFO NVLS multicast support is available on dev 1
worker-1:5868:6003 [1] NCCL INFO NVLS Head  0:  0  8
worker-1:5868:6003 [1] NCCL INFO NVLS Head  1:  1  9
worker-1:5868:6003 [1] NCCL INFO NVLS Head  2:  2 10
worker-1:5868:6003 [1] NCCL INFO NVLS Head  3:  3 11
worker-1:5868:6003 [1] NCCL INFO NVLS Head  4:  4 12
worker-1:5868:6003 [1] NCCL INFO NVLS Head  5:  5 13
worker-1:5868:6003 [1] NCCL INFO NVLS Head  6:  6 14
worker-1:5868:6003 [1] NCCL INFO NVLS Head  7:  7 15
worker-0:5958:6037 [2] NCCL INFO Channel 09/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 10/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/9/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->9 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
worker-1:5868:6003 [1] NCCL INFO P2P Chunksize set to 131072
worker-1:5868:6003 [1] NCCL INFO Channel 01/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 09/0 : 10[2] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [send] via NET/IBext/4(0)/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-0:5953:5953 [0] NCCL INFO cudaDriverVersion 12030
worker-0:5953:5953 [0] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5953:5953 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5953:5953 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5953:5953 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5953:5953 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5953:6033 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5953:6033 [0] NCCL INFO P2P plugin IBext
worker-0:5953:6033 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5953:6033 [0] NCCL INFO Using non-device net plugin version 0
worker-0:5953:6033 [0] NCCL INFO Using network IBext
worker-1:5868:6003 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Connected all rings
worker-1:5868:6003 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO comm 0x55b5f22a6530 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x906175c8434fff47 - Init START
worker-0:5953:6033 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5953:6033 [0] NCCL INFO Setting affinity for GPU 0 to 01
worker-0:5953:6033 [0] NCCL INFO NVLS multicast support is available on dev 0
worker-0:5953:6033 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] -1/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/-1/-1->8->15 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15 [8] 9/0/-1->8->-1 [9] -1/-1/-1->8->15 [10] 9/-1/-1->8->15 [11] 9/-1/-1->8->15 [12] 9/-1/-1->8->15 [13] 9/-1/-1->8->15 [14] 9/-1/-1->8->15 [15] 9/-1/-1->8->15
worker-0:5953:6033 [0] NCCL INFO P2P Chunksize set to 131072
worker-0:5953:6033 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 08/0 : 1[1] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 08/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 09/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 10/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 11/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 12/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 13/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-1:5867:5867 [3] NCCL INFO cudaDriverVersion 12030
worker-1:5867:5867 [3] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5867:5867 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5867:5867 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5867:5867 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5867:5867 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5867:6001 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5867:6001 [3] NCCL INFO P2P plugin IBext
worker-1:5867:6001 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5867:6001 [3] NCCL INFO Using non-device net plugin version 0
worker-1:5867:6001 [3] NCCL INFO Using network IBext
worker-0:5953:6033 [0] NCCL INFO Channel 14/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [send] via NET/IBext/3(15)/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Connected all rings
worker-0:5953:6033 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 08/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 10/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO comm 0x55a4db83bda0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x906175c8434fff47 - Init START
worker-1:5867:6001 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5867:6001 [3] NCCL INFO Setting affinity for GPU 3 to 08
worker-1:5867:6001 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-1:5867:6001 [3] NCCL INFO NVLS Head  0:  0  8
worker-1:5867:6001 [3] NCCL INFO NVLS Head  1:  1  9
worker-1:5867:6001 [3] NCCL INFO NVLS Head  2:  2 10
worker-1:5867:6001 [3] NCCL INFO NVLS Head  3:  3 11
worker-1:5867:6001 [3] NCCL INFO NVLS Head  4:  4 12
worker-1:5867:6001 [3] NCCL INFO NVLS Head  5:  5 13
worker-1:5867:6001 [3] NCCL INFO NVLS Head  6:  6 14
worker-1:5867:6001 [3] NCCL INFO NVLS Head  7:  7 15
worker-0:5953:6033 [0] NCCL INFO Channel 11/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/11/-1->3->-1 [4] -1/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->11 [12] -1/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
worker-1:5867:6001 [3] NCCL INFO P2P Chunksize set to 131072
worker-1:5867:6001 [3] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [send] via NET/IBext/6(2)/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-0:5955:5955 [6] NCCL INFO cudaDriverVersion 12030
worker-0:5955:5955 [6] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5955:5955 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5955:5955 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5955:5955 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5955:5955 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5955:6038 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5955:6038 [6] NCCL INFO P2P plugin IBext
worker-0:5955:6038 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5955:6038 [6] NCCL INFO Using non-device net plugin version 0
worker-0:5955:6038 [6] NCCL INFO Using network IBext
worker-1:5867:6001 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Connected all rings
worker-1:5867:6001 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO comm 0x56196e6561a0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x906175c8434fff47 - Init START
worker-0:5955:6038 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5955:6038 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-0:5955:6038 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->6 [7] -1/-1/-1->14->13 [8] 15/-1/-1->14->13 [9] 15/-1/-1->14->13 [10] 15/-1/-1->14->13 [11] 15/-1/-1->14->13 [12] 15/-1/-1->14->13 [13] 15/-1/-1->14->13 [14] 15/6/-1->14->-1 [15] -1/-1/-1->14->13
worker-0:5955:6038 [6] NCCL INFO P2P Chunksize set to 131072
worker-0:5955:6038 [6] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [send] via NET/IBext/1(13)/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 03/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 06/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 07/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 08/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 09/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 10/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-1:5874:5874 [5] NCCL INFO cudaDriverVersion 12030
worker-1:5874:5874 [5] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5874:5874 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5874:5874 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5874:5874 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5874:5874 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5874:5999 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5874:5999 [5] NCCL INFO P2P plugin IBext
worker-1:5874:5999 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5874:5999 [5] NCCL INFO Using non-device net plugin version 0
worker-1:5874:5999 [5] NCCL INFO Using network IBext
worker-0:5955:6038 [6] NCCL INFO Channel 11/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 12/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 14/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 15/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Connected all rings
worker-0:5955:6038 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 08/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO comm 0x55921dd4ce70 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x906175c8434fff47 - Init START
worker-1:5874:5999 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5874:5999 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-1:5874:5999 [5] NCCL INFO NVLS Head  0:  0  8
worker-1:5874:5999 [5] NCCL INFO NVLS Head  1:  1  9
worker-1:5874:5999 [5] NCCL INFO NVLS Head  2:  2 10
worker-1:5874:5999 [5] NCCL INFO NVLS Head  3:  3 11
worker-1:5874:5999 [5] NCCL INFO NVLS Head  4:  4 12
worker-1:5874:5999 [5] NCCL INFO NVLS Head  5:  5 13
worker-1:5874:5999 [5] NCCL INFO NVLS Head  6:  6 14
worker-1:5874:5999 [5] NCCL INFO NVLS Head  7:  7 15
worker-0:5955:6038 [6] NCCL INFO Channel 09/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 10/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/13/-1->5->-1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->13 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4
worker-1:5874:5999 [5] NCCL INFO P2P Chunksize set to 131072
worker-1:5874:5999 [5] NCCL INFO Channel 05/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 13/0 : 14[6] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [send] via NET/IBext/0(4)/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-0:5956:5956 [5] NCCL INFO cudaDriverVersion 12030
worker-0:5956:5956 [5] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5956:5956 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5956:5956 [5] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5956:5956 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5956:5956 [5] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5956:6032 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5956:6032 [5] NCCL INFO P2P plugin IBext
worker-0:5956:6032 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5956:6032 [5] NCCL INFO Using non-device net plugin version 0
worker-0:5956:6032 [5] NCCL INFO Using network IBext
worker-1:5874:5999 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Connected all rings
worker-1:5874:5999 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO comm 0x556c2109acb0 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x906175c8434fff47 - Init START
worker-0:5956:6032 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5956:6032 [5] NCCL INFO NVLS multicast support is available on dev 5
worker-0:5956:6032 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->5 [6] -1/-1/-1->13->12 [7] 14/-1/-1->13->12 [8] 14/-1/-1->13->12 [9] 14/-1/-1->13->12 [10] 14/-1/-1->13->12 [11] 14/-1/-1->13->12 [12] 14/-1/-1->13->12 [13] 14/5/-1->13->-1 [14] -1/-1/-1->13->12 [15] 14/-1/-1->13->12
worker-0:5956:6032 [5] NCCL INFO P2P Chunksize set to 131072
worker-0:5956:6032 [5] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [send] via NET/IBext/0(12)/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 05/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 08/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 09/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 10/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-1:5876:5876 [4] NCCL INFO cudaDriverVersion 12030
worker-1:5876:5876 [4] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5876:5876 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5876:5876 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5876:5876 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5876:5876 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5876:6004 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5876:6004 [4] NCCL INFO P2P plugin IBext
worker-1:5876:6004 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5876:6004 [4] NCCL INFO Using non-device net plugin version 0
worker-1:5876:6004 [4] NCCL INFO Using network IBext
worker-0:5956:6032 [5] NCCL INFO Channel 11/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 13/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 14/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 15/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Connected all rings
worker-0:5956:6032 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 03/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 07/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 08/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO comm 0x5558f7e3e070 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x906175c8434fff47 - Init START
worker-1:5876:6004 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5876:6004 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-1:5876:6004 [4] NCCL INFO NVLS Head  0:  0  8
worker-1:5876:6004 [4] NCCL INFO NVLS Head  1:  1  9
worker-1:5876:6004 [4] NCCL INFO NVLS Head  2:  2 10
worker-1:5876:6004 [4] NCCL INFO NVLS Head  3:  3 11
worker-1:5876:6004 [4] NCCL INFO NVLS Head  4:  4 12
worker-1:5876:6004 [4] NCCL INFO NVLS Head  5:  5 13
worker-1:5876:6004 [4] NCCL INFO NVLS Head  6:  6 14
worker-1:5876:6004 [4] NCCL INFO NVLS Head  7:  7 15
worker-0:5956:6032 [5] NCCL INFO Channel 09/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 10/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/12/-1->4->-1 [5] -1/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->12 [13] -1/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
worker-1:5876:6004 [4] NCCL INFO P2P Chunksize set to 131072
worker-1:5876:6004 [4] NCCL INFO Channel 04/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 12/0 : 13[5] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [send] via NET/IBext/7(3)/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-0:5967:5967 [3] NCCL INFO cudaDriverVersion 12030
worker-0:5967:5967 [3] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5967:5967 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5967:5967 [3] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5967:5967 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5967:5967 [3] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5967:6035 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5967:6035 [3] NCCL INFO P2P plugin IBext
worker-0:5967:6035 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5967:6035 [3] NCCL INFO Using non-device net plugin version 0
worker-0:5967:6035 [3] NCCL INFO Using network IBext
worker-1:5876:6004 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Connected all rings
worker-1:5876:6004 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO comm 0x55fc9880c490 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x906175c8434fff47 - Init START
worker-0:5967:6035 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5967:6035 [3] NCCL INFO Setting affinity for GPU 3 to 08
worker-0:5967:6035 [3] NCCL INFO NVLS multicast support is available on dev 3
worker-0:5967:6035 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->3 [4] -1/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] 12/-1/-1->11->10 [7] 12/-1/-1->11->10 [8] 12/-1/-1->11->10 [9] 12/-1/-1->11->10 [10] 12/-1/-1->11->10 [11] 12/3/-1->11->-1 [12] -1/-1/-1->11->10 [13] 12/-1/-1->11->10 [14] 12/-1/-1->11->10 [15] 12/-1/-1->11->10
worker-0:5967:6035 [3] NCCL INFO P2P Chunksize set to 131072
worker-0:5967:6035 [3] NCCL INFO Channel 03/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 11/0 : 4[4] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 02/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 10/0 : 11[3] -> 2[2] [send] via NET/IBext/6(10)/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 08/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 09/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-1:5871:5871 [6] NCCL INFO cudaDriverVersion 12030
worker-1:5871:5871 [6] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5871:5871 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5871:5871 [6] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5871:5871 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5871:5871 [6] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5871:5998 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5871:5998 [6] NCCL INFO P2P plugin IBext
worker-1:5871:5998 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5871:5998 [6] NCCL INFO Using non-device net plugin version 0
worker-1:5871:5998 [6] NCCL INFO Using network IBext
worker-0:5967:6035 [3] NCCL INFO Channel 11/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 12/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 13/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 14/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 15/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Connected all rings
worker-0:5967:6035 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 06/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO comm 0x560f2f10c050 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x906175c8434fff47 - Init START
worker-1:5871:5998 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5871:5998 [6] NCCL INFO NVLS multicast support is available on dev 6
worker-1:5871:5998 [6] NCCL INFO NVLS Head  0:  0  8
worker-1:5871:5998 [6] NCCL INFO NVLS Head  1:  1  9
worker-1:5871:5998 [6] NCCL INFO NVLS Head  2:  2 10
worker-1:5871:5998 [6] NCCL INFO NVLS Head  3:  3 11
worker-1:5871:5998 [6] NCCL INFO NVLS Head  4:  4 12
worker-1:5871:5998 [6] NCCL INFO NVLS Head  5:  5 13
worker-1:5871:5998 [6] NCCL INFO NVLS Head  6:  6 14
worker-1:5871:5998 [6] NCCL INFO NVLS Head  7:  7 15
worker-0:5967:6035 [3] NCCL INFO Channel 08/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 09/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/14/-1->6->-1 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->14 [15] -1/-1/-1->6->5
worker-1:5871:5998 [6] NCCL INFO P2P Chunksize set to 131072
worker-1:5871:5998 [6] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [send] via NET/IBext/1(5)/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-0:5954:5954 [4] NCCL INFO cudaDriverVersion 12030
worker-0:5954:5954 [4] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5954:5954 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5954:5954 [4] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5954:5954 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5954:5954 [4] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5954:6031 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5954:6031 [4] NCCL INFO P2P plugin IBext
worker-0:5954:6031 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5954:6031 [4] NCCL INFO Using non-device net plugin version 0
worker-0:5954:6031 [4] NCCL INFO Using network IBext
worker-1:5871:5998 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Connected all rings
worker-1:5871:5998 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO comm 0x558b20301640 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x906175c8434fff47 - Init START
worker-0:5954:6031 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5954:6031 [4] NCCL INFO NVLS multicast support is available on dev 4
worker-0:5954:6031 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->4 [5] -1/-1/-1->12->11 [6] 13/-1/-1->12->11 [7] 13/-1/-1->12->11 [8] 13/-1/-1->12->11 [9] 13/-1/-1->12->11 [10] 13/-1/-1->12->11 [11] 13/-1/-1->12->11 [12] 13/4/-1->12->-1 [13] -1/-1/-1->12->11 [14] 13/-1/-1->12->11 [15] 13/-1/-1->12->11
worker-0:5954:6031 [4] NCCL INFO P2P Chunksize set to 131072
worker-0:5954:6031 [4] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 03/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 11/0 : 12[4] -> 3[3] [send] via NET/IBext/7(11)/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 08/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 09/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 10/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-1:5875:5875 [7] NCCL INFO cudaDriverVersion 12030
worker-1:5875:5875 [7] NCCL INFO Bootstrap : Using eth0:10.113.5.94<0>
worker-1:5875:5875 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-1:5875:5875 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-1:5875:5875 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-1:5875:5875 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-1:5875:6000 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-1:5875:6000 [7] NCCL INFO P2P plugin IBext
worker-1:5875:6000 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.5.94<0>
worker-1:5875:6000 [7] NCCL INFO Using non-device net plugin version 0
worker-1:5875:6000 [7] NCCL INFO Using network IBext
worker-0:5954:6031 [4] NCCL INFO Channel 12/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 13/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 14/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 15/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Connected all rings
worker-0:5954:6031 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 08/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO comm 0x559374a86e20 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x906175c8434fff47 - Init START
worker-1:5875:6000 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-1:5875:6000 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-1:5875:6000 [7] NCCL INFO NVLS Head  0:  0  8
worker-1:5875:6000 [7] NCCL INFO NVLS Head  1:  1  9
worker-1:5875:6000 [7] NCCL INFO NVLS Head  2:  2 10
worker-1:5875:6000 [7] NCCL INFO NVLS Head  3:  3 11
worker-1:5875:6000 [7] NCCL INFO NVLS Head  4:  4 12
worker-1:5875:6000 [7] NCCL INFO NVLS Head  5:  5 13
worker-1:5875:6000 [7] NCCL INFO NVLS Head  6:  6 14
worker-1:5875:6000 [7] NCCL INFO NVLS Head  7:  7 15
worker-0:5954:6031 [4] NCCL INFO Channel 09/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 10/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] 0/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/15/-1->7->-1 [8] -1/-1/-1->7->6 [9] 0/-1/-1->7->6 [10] 0/-1/-1->7->6 [11] 0/-1/-1->7->6 [12] 0/-1/-1->7->6 [13] 0/-1/-1->7->6 [14] 0/-1/-1->7->6 [15] 0/-1/-1->7->15
worker-1:5875:6000 [7] NCCL INFO P2P Chunksize set to 131072
worker-1:5875:6000 [7] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [send] via NET/IBext/2(6)/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 07/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 15/0 : 8[0] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-0:5960:5960 [7] NCCL INFO cudaDriverVersion 12030
worker-0:5960:5960 [7] NCCL INFO Bootstrap : Using eth0:10.113.4.230<0>
worker-0:5960:5960 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.
worker-0:5960:5960 [7] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)
worker-0:5960:5960 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
worker-0:5960:5960 [7] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)
worker-0:5960:6036 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
worker-0:5960:6036 [7] NCCL INFO P2P plugin IBext
worker-0:5960:6036 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP [2]mlx5_2:1/IB/SHARP [3]mlx5_3:1/IB/SHARP [4]mlx5_4:1/IB/SHARP [5]mlx5_5:1/IB/SHARP [6]mlx5_6:1/IB/SHARP [7]mlx5_7:1/IB/SHARP [RO]; OOB eth0:10.113.4.230<0>
worker-0:5960:6036 [7] NCCL INFO Using non-device net plugin version 0
worker-0:5960:6036 [7] NCCL INFO Using network IBext
worker-1:5875:6000 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Connected all rings
worker-1:5875:6000 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO comm 0x55746e322500 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x906175c8434fff47 - Init START
worker-0:5960:6036 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /var/run/nvidia-topologyd/virtualTopology.xml
worker-0:5960:6036 [7] NCCL INFO NVLS multicast support is available on dev 7
worker-0:5960:6036 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] 8/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->7 [8] -1/-1/-1->15->14 [9] 8/-1/-1->15->14 [10] 8/-1/-1->15->14 [11] 8/-1/-1->15->14 [12] 8/-1/-1->15->14 [13] 8/-1/-1->15->14 [14] 8/-1/-1->15->14 [15] 8/7/-1->15->-1
worker-0:5960:6036 [7] NCCL INFO P2P Chunksize set to 131072
worker-0:5960:6036 [7] NCCL INFO Channel 06/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 14/0 : 15[7] -> 6[6] [send] via NET/IBext/2(14)/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 07/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 08/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 09/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 10/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 11/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 12/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 13/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 15/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Connected all rings
worker-0:5960:6036 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 04/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 08/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 11/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 12/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 13/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 14/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 07/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 15/0 : 0[0] -> 15[7] [send] via NET/IBext/3(7)/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Connected all rings
worker-1:5881:6002 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[7] via P2P/CUMEM
worker-1:5881:6002 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Connected all trees
worker-1:5881:6002 [0] NCCL INFO NVLS comm 0x55e6f6d3f030 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5881:6002 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 14/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [receive] via NET/IBext/4/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM
worker-1:5874:5999 [5] NCCL INFO Connected all trees
worker-1:5874:5999 [5] NCCL INFO NVLS comm 0x55921dd4ce70 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5874:5999 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [receive] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 14/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [send] via NET/IBext/1/GDRDMA
worker-1:5874:5999 [5] NCCL INFO Connected NVLS tree
worker-1:5874:5999 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM
worker-1:5871:5998 [6] NCCL INFO Connected all trees
worker-1:5871:5998 [6] NCCL INFO NVLS comm 0x560f2f10c050 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5871:5998 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [receive] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [send] via NET/IBext/2/GDRDMA
worker-1:5871:5998 [6] NCCL INFO Connected NVLS tree
worker-1:5871:5998 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5868:6003 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5868:6003 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
worker-1:5868:6003 [1] NCCL INFO Connected all trees
worker-1:5868:6003 [1] NCCL INFO NVLS comm 0x55b68d4b1ed0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5868:6003 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5868:6003 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [receive] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [send] via NET/IBext/5/GDRDMA
worker-1:5868:6003 [1] NCCL INFO Connected NVLS tree
worker-1:5876:6004 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM
worker-1:5876:6004 [4] NCCL INFO Connected all trees
worker-1:5876:6004 [4] NCCL INFO NVLS comm 0x5558f7e3e070 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5876:6004 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [receive] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [send] via NET/IBext/0/GDRDMA
worker-1:5876:6004 [4] NCCL INFO Connected NVLS tree
worker-1:5876:6004 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:5873:6005 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
worker-1:5873:6005 [2] NCCL INFO Connected all trees
worker-1:5873:6005 [2] NCCL INFO NVLS comm 0x55a42bc47700 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5873:6005 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [receive] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [send] via NET/IBext/6/GDRDMA
worker-1:5873:6005 [2] NCCL INFO Connected NVLS tree
worker-1:5867:6001 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5867:6001 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
worker-1:5867:6001 [3] NCCL INFO Connected all trees
worker-1:5867:6001 [3] NCCL INFO NVLS comm 0x55a4db83bda0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5867:6001 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5867:6001 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [receive] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5867:6001 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5867:6001 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [send] via NET/IBext/7/GDRDMA
worker-1:5867:6001 [3] NCCL INFO Connected NVLS tree
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5875:6000 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5875:6000 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM
worker-1:5875:6000 [7] NCCL INFO Connected all trees
worker-1:5875:6000 [7] NCCL INFO NVLS comm 0x559374a86e20 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5875:6000 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5875:6000 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [receive] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5875:6000 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
Configure sharded model for LatentDiffusion
worker-1:5875:6000 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [send] via NET/IBext/3/GDRDMA
worker-1:5875:6000 [7] NCCL INFO Connected NVLS tree
worker-1:5875:6000 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
Deleting key model.diffusion_model.out.0.bias from state_dict.
Deleting key model.diffusion_model.out.2.weight from state_dict.
Deleting key model.diffusion_model.out.2.bias from state_dict.
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
Missing Keys:
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
building MemoryEfficientAttnBlock with 512 in_channels...
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
building MemoryEfficientAttnBlock with 512 in_channels...
Deleting key model.diffusion_model.time_embed.0.weight from state_dict.
Deleting key model.diffusion_model.time_embed.0.bias from state_dict.
Deleting key model.diffusion_model.time_embed.2.weight from state_dict.
Deleting key model.diffusion_model.time_embed.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.0.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.1.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.2.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.3.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.6.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.9.0.op.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.input_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.norm.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.middle_block.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.middle_block.2.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.0.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.1.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.2.1.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.3.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.4.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.5.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.6.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.7.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.8.2.conv.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.9.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.10.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.in_layers.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.emb_layers.1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.out_layers.3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.0.skip_connection.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.norm.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_in.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.weight from state_dict.
Deleting key model.diffusion_model.output_blocks.11.1.proj_out.bias from state_dict.
Deleting key model.diffusion_model.out.0.weight from state_dict.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.out.0.bias from state_dict.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.out.2.weight from state_dict.
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Deleting key model.diffusion_model.out.2.bias from state_dict.
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Missing Keys:
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_bloLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
cks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias']

Unexpected Keys:
 ['model_ema.decay', 'model_ema.num_updates']
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
worker-1:5881:6002 [0] NCCL INFO Channel 01/0 Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
:::MLLOG {"namespace": "", "time_ms": 1718480550942, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1639}}
:::MLLOG {"namespace": "", "time_ms": 1718480551121, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1640}}
:::MLLOG {"namespace": "", "time_ms": 1718480551123, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1641}}
:::MLLOG {"namespace": "", "time_ms": 1718480551125, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1642}}
:::MLLOG {"namespace": "", "time_ms": 1718480551126, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1643}}
:::MLLOG {"namespace": "", "time_ms": 1718480551127, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 1.6e-05, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1644}}
:::MLLOG {"namespace": "", "time_ms": 1718480551130, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "ldm/models/diffusion/ddpm.py", "lineno": 1650}}
Setting up LambdaLR scheduler...
Project config
model:
  base_learning_rate: 1.25e-07
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    parameterization: v
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: npy
    first_stage_type: moments
    cond_stage_key: txt
    image_size: 64
    channels: 4
    cond_stage_trainable: false
    conditioning_key: crossattn
    monitor: steps
    scale_factor: 0.18215
    use_ema: false
    load_vae: true
    load_unet: false
    load_encoder: true
    validation_config:
      sampler: ddim
      steps: 50
      scale: 8.0
      ddim_eta: 0.0
      prompt_key: caption
      image_fname_key: image_id
      save_images:
        enabled: false
        base_output_dir: /results/inference
      fid:
        enabled: true
        inception_weights_url: https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth
        cache_dir: /checkpoints/inception
        gt_path: /datasets/coco2014/val2014_30k_stats.npz
      clip:
        enabled: true
        clip_version: ViT-H-14
        cache_dir: /checkpoints/clip
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 1000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        use_checkpoint: false
        use_fp16: true
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        num_head_channels: 64
        use_spatial_transformer: true
        use_linear_in_transformer: true
        transformer_depth: 1
        context_dim: 1024
        legacy: false
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder
      params:
        arch: ViT-H-14
        version: laion2b_s32b_b79k
        freeze: true
        layer: penultimate
        cache_dir: /checkpoints/clip
    use_fp16: true
    ckpt: /checkpoints/sd/512-base-ema.ckpt
data:
  target: ldm.data.composable_data_module.ComposableDataModule
  params:
    train:
      target: ldm.data.webdatasets.build_dataloader
      params:
        urls: /datasets/laion-400m/webdataset-moments-filtered/{00000..00831}.tar
        batch_size: 8
        shuffle: 1000
        partial: false
        keep_only_keys:
        - npy
        - txt
        num_workers: 4
        persistent_workers: true
    validation:
      target: ldm.data.tsv.build_dataloader
      params:
        annotations_file: /datasets/coco2014/val2014_30k.tsv
        keys:
        - image_id
        - id
        - caption
        batch_size: 8
        shuffle: false
        num_workers: 1

Lightning config
trainer:
  accelerator: gpu
  num_nodes: 2
  devices: 8
  precision: 16
  logger: false
  log_every_n_steps: 5
  enable_progress_bar: false
  max_epochs: -1
  max_steps: 10000000000000
  val_check_interval: 1000
  enable_checkpointing: true
  num_sanity_val_steps: 0
  strategy:
    target: strategies.DDPStrategy
    params:
      find_unused_parameters: false
modelcheckpoint:
  target: lightning.pytorch.callbacks.ModelCheckpoint
  params:
    save_top_k: -1
    every_n_train_steps: 1000

:::MLLOG {"namespace": "", "time_ms": 1718480551206, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "mlperf_logging_utils.py", "lineno": 88}}

  | Name              | Type                   | Params
-------------------------------------------------------------
0 | model             | DiffusionWrapper       | 865 M 
1 | first_stage_model | AutoencoderKL          | 83.7 M
2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
-------------------------------------------------------------
865 M     Trainable params
437 M     Non-trainable params
1.3 B     Total params
2,607.194 Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.
  rank_zero_warn(
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5868:6003 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 51Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:5959:6034 [1] NCCL INFO Channel 12/0 : 9[1] -> 10[2Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
2
worker-1:5868:6003 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5868:6003 [1] NCCL INFO comm 0x55b68d4b1ed0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x906175c8434fff47 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 13/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 14/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 15/0 : 9[1] -> 10[2] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Channel 08/0 : 9[1] -> 8[0] via P2P/CUMEM
worker-0:5959:6034 [1] NCCL INFO Connected all trees
worker-0:5959:6034 [1] NCCL INFO NVLS comm 0x55cf8569d7c0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:5959:6034 [1] NCCL INFO Channel 00/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 02/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 04/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 06/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 08/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 10/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 12/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 14/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [receive] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 00/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 02/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 04/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 06/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-1:5867:6001 [3] NCCL INFO threadThrSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5959:6034 [1] NCCL INFO Channel 08/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 10/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 12/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 14/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [send] via NET/IBext/5/GDRDMA
worker-0:5959:6034 [1] NCCL INFO Connected NVLS tree
worker-0:5959:6034 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5959:6034 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5959:6034 [1] NCCL INFO comm 0x55cf8569d7c0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 91000 commId 0x906175c8434fff47 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:5958:6Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
esholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:5867:6001 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5867:6001 [3] NCCL INFO comm 0x55a4db83bda0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x906175c8434fff47 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5874:5999 [5] NCCL Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
037 [2] NCCL INFO Channel 12/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 13/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 14/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 15/0 : 10[2] -> 11[3] via P2P/CUMEM
worker-0:5958:6037 [2] NCCL INFO Channel 02/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 02/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5958:6037 [2] NCCL INFO Channel 09/0 : 10[2] -> 9[1] via P2P/CUMEM
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:5958:6037 [2] NCCL INFO Connected all trees
worker-0:5958:6037 [2] NCCL INFO NVLS comm 0x55ecc7cdcfd0 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:5958:6037 [2] NCCL INFO Channel 00/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 04/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 06/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 08/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:5958:6037 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 12/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 14/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [receive] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 00/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 04/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5874:5999 [5] NCCL INFO comm 0x55921dd4ce70 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5958:6037 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 06/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 08/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 12/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 14/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [send] via NET/IBext/6/GDRDMA
worker-0:5958:6037 [2] NCCL INFO Connected NVLS tree
worker-1:5876:6004 [4] NCCL Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5958:6037 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5958:6037 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:5958:6037 [2] NCCL INFO comm 0x55ecc7cdcfd0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x906175c8434fff47 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:5956:6032 [5] NCCL INFO Channel 11/0 : 13[Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5876:6004 [4] NCCL INFO comm 0x5558f7e3e070 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x906175c8434fff47 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5871:5998 [6] NCCL Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 160 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Training is starting
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 12/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 13/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 15/0 : 13[5] -> 14[6] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Channel 12/0 : 13[5] -> 12[4] via P2P/CUMEM
worker-0:5956:6032 [5] NCCL INFO Connected all trees
worker-0:5956:6032 [5] NCCL INFO NVLS comm 0x556c2109acb0 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:5956:6032 [5] NCCL INFO Channel 00/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 02/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 04/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 06/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 08/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 10/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:5956:6032 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 12/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 14/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [receive] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 00/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 02/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 04/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 06/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5871:5998 [6] NCCL INFO comm 0x560f2f10c050 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5956:6032 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 08/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 10/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 12/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 14/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [send] via NET/IBext/1/GDRDMA
worker-0:5956:6032 [5] NCCL INFO Connected NVLS tree
worker-0:5956:6032 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5956:6032 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-1:5875:6000 [7] NCCL INSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
worker-0:5956:6032 [5] NCCL INFO comm 0x556c2109acb0 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId af000 commId 0x906175c8434fff47 - Init COMPLETE
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5954:6031 [4] NCCL INFO Channel 11/0 : 12[Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
FO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5875:6000 [7] NCCL INFO comm 0x559374a86e20 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x906175c8434fff47 - Init COMPLETE
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 12/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 14/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 15/0 : 12[4] -> 13[5] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 04/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 04/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Channel 11/0 : 12[4] -> 11[3] via P2P/CUMEM
worker-0:5954:6031 [4] NCCL INFO Connected all trees
worker-0:5954:6031 [4] NCCL INFO NVLS comm 0x558b20301640 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-1:5873:6005 [2] NCCL INFO threadThresSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5954:6031 [4] NCCL INFO Channel 00/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 02/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 06/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 08/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 10/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
worker-0:5954:6031 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 14/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [receive] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 00/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 02/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 06/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
worker-0:5954:6031 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 08/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 10/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 14/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [send] via NET/IBext/0/GDRDMA
worker-0:5954:6031 [4] NCCL INFO Connected NVLS tree
worker-0:5954:6031 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5954:6031 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-0:5954:6031 [4] NCCL INFO comm 0x558b20301640 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId ab000 commId 0x906175c8434fff47 - Init COMPLETE
holds 8/8/64 | 128/8/64 | 512 | 512
worker-1:5873:6005 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5873:6005 [2] NCCL INFO comm 0x55a42bc47700 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 95000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5953:6033 [0] NCCL INFO Channel 12/0 : 8[0] -Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
: 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
worker-1:5881:6002 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [send] via NET/IBext/4/GDRDMA
worker-1:5881:6002 [0] NCCL INFO Connected NVLS tree
worker-1:5881:6002 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-1:5881:6002 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-1:5881:6002 [0] NCCL INFO comm 0x55e6f6d3f030 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x906175c8434fff47 - Init COMPLETE
> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 13/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 14/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 15/0 : 8[0] -> 9[1] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 15/0 : 8[0] -> 15[7] via P2P/CUMEM
worker-0:5953:6033 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Connected all trees
worker-0:5953:6033 [0] NCCL INFO NVLS comm 0x55b5f22a6530 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:5953:6033 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 02/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 04/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 06/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 10/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 12/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 14/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [receive] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 02/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 04/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 06/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 10/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 12/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 14/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [send] via NET/IBext/4/GDRDMA
worker-0:5953:6033 [0] NCCL INFO Connected NVLS tree
worker-0:5953:6033 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5953:6033 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:5953:6033 [0] NCCL INFO comm 0x55b5f22a6530 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 8d000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5967:6035 [3] NCCL INFO Channel 10/0 : 11[3] -> 12[4]Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
 via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 11/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 13/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 14/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 15/0 : 11[3] -> 12[4] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Channel 10/0 : 11[3] -> 10[2] via P2P/CUMEM
worker-0:5967:6035 [3] NCCL INFO Connected all trees
worker-0:5967:6035 [3] NCCL INFO NVLS comm 0x55fc9880c490 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:5967:6035 [3] NCCL INFO Channel 00/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 02/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 04/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 06/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 08/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 10/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 12/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 14/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [receive] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 00/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 02/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 04/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 06/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 08/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 10/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 12/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 14/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [send] via NET/IBext/7/GDRDMA
worker-0:5967:6035 [3] NCCL INFO Connected NVLS tree
worker-0:5967:6035 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5967:6035 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:5967:6035 [3] NCCL INFO comm 0x55fc9880c490 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 99000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5955:6038 [6] NCCL INFO Channel 11/0 : 14[Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 12/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 13/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 14/0 : 14[6] -> 15[7] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 06/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 06/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/CUMEM
worker-0:5955:6038 [6] NCCL INFO Channel 13/0 : 14[6] -> 13[5] via P2P/CUMEM
workeSetting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
DiffusionWrapper has 865.91 M params.
r-0:5960:6036 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 09/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 10/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 11/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 12/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 13/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 14/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 15/0 : 15[7] -> 8[0] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Channel 14/0 : 15[7] -> 14[6] via P2P/CUMEM
worker-0:5960:6036 [7] NCCL INFO Connected all trees
worker-0:5955:6038 [6] NCCL INFO Connected all trees
worker-0:5955:6038 [6] NCCL INFO NVLS comm 0x56196e6561a0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:5955:6038 [6] NCCL INFO Channel 00/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 02/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 04/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 08/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 10/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 12/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [receive] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 00/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 02/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 04/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 08/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 10/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 12/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [send] via NET/IBext/2/GDRDMA
worker-0:5955:6038 [6] NCCL INFO Connected NVLS tree
worker-0:5955:6038 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5955:6038 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:5955:6038 [6] NCCL INFO comm 0x56196e6561a0 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId b3000 commId 0x906175c8434fff47 - Init COMPLETE
worker-0:5960:6036 [7] NCCL INFO NVLS comm 0x55746e322500 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
worker-0:5960:6036 [7] NCCL INFO Channel 00/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 02/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 04/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 06/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 08/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 10/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 12/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 14/0 : 7[7] -> 15[7] [receive] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 00/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 02/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 04/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 06/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 08/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 10/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 12/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Channel 14/0 : 15[7] -> 7[7] [send] via NET/IBext/3/GDRDMA
worker-0:5960:6036 [7] NCCL INFO Connected NVLS tree
worker-0:5960:6036 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
worker-0:5960:6036 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
worker-0:5960:6036 [7] NCCL INFO comm 0x55746e322500 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId b7000 commId 0x906175c8434fff47 - Init COMPLETE
:::MLLOG {"namespace": "", "time_ms": 1718480554668, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "mlperf_logging_utils.py", "lineno": 108, "step_num": 0}}
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
[rank15]:[W CUDAGuardImpl.h:115] Warning: CUDA warning: unspecified launch failure (function destroyEvent)
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /opt/pytorch/pytorch/c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f6c80e858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f6c80e3abb6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f6c8b1d2e12 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x1914e (0x7f6c8b1a014e in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1b7ed (0x7f6c8b1a27ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x1bbc5 (0x7f6c8b1a2bc5 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)
frame #6: <unknown function> + 0x483b00 (0x7f6c7fdf0b00 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f6c80e61419 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #8: c10d::Reducer::~Reducer() + 0x32a (0x7f6c7998be1a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xbaf2a2 (0x7f6c8051c2a2 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x3af29a (0x7f6c7fd1c29a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #11: <unknown function> + 0xbb3a51 (0x7f6c80520a51 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #12: <unknown function> + 0x3b9cdd (0x7f6c7fd26cdd in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
frame #13: <unknown function> + 0x3babc1 (0x7f6c7fd27bc1 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #44: <unknown function> + 0x29d90 (0x7f6c8ff9cd90 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #45: __libc_start_main + 0x80 (0x7f6c8ff9ce40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank12]:[E ProcessGroupNCCL.cpp:754] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
[rank13]:[E ProcessGroupNCCL.cpp:754] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
[rank12]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank12]:[E ProcessGroupNCCL.cpp:1282] [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5192f9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f5134d58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f5134d5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f5134d5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f5192ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f51a21a3ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f51a2235a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank13]:[E ProcessGroupNCCL.cpp:1282] [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f82a3d9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f8245b58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f8245b5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f8245b5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f82a38b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f82b2f6bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f82b2ffda40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 13] NCCL watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f82a3d9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f8245b58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f8245b5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f8245b5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f82a38b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f82b2f6bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f82b2ffda40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f82a3d9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f8245b86d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f82458be879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f82a38b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f82b2f6bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f82b2ffda40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

  what():  [Rank 12] NCCL watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800902 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5192f9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f5134d58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f5134d5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f5134d5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f5192ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f51a21a3ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f51a2235a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5192f9c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f5134d86d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f5134abe879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f5192ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f51a21a3ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f51a2235a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank9]:[E ProcessGroupNCCL.cpp:754] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800907 milliseconds before timing out.
[rank8]:[E ProcessGroupNCCL.cpp:754] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800906 milliseconds before timing out.
[rank9]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E ProcessGroupNCCL.cpp:1282] [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800906 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2dabe858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f2d4dd58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f2d4dd5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f2d4dd5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f2dabab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f2dbb077ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f2dbb109a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank9]:[E ProcessGroupNCCL.cpp:1282] [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800907 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f79222858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f78c4158142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f78c415e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f78c415eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f7921eb0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7931487ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f7931519a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 8] NCCL watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800906 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2dabe858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f2d4dd58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f2d4dd5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f2d4dd5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f2dabab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f2dbb077ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f2dbb109a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f2dabe858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f2d4dd86d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f2d4dabe879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f2dabab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f2dbb077ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f2dbb109a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

  what():  [Rank 9] NCCL watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800907 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f79222858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f78c4158142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f78c415e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f78c415eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f7921eb0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f7931487ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f7931519a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f79222858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f78c4186d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f78c3ebe879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f7921eb0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f7931487ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f7931519a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank10]:[E ProcessGroupNCCL.cpp:754] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800933 milliseconds before timing out.
[rank11]:[E ProcessGroupNCCL.cpp:754] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800939 milliseconds before timing out.
[rank10]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank10]:[E ProcessGroupNCCL.cpp:1282] [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800933 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5cca99c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f5c6c758142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f5c6c75e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f5c6c75eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f5cca4b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f5cd9b0fac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f5cd9ba1a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank14]:[E ProcessGroupNCCL.cpp:754] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800941 milliseconds before timing out.
[rank11]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E ProcessGroupNCCL.cpp:1282] [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800939 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcab2e858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7fca54d58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7fca54d5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7fca54d5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fcab2ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fcac1fedac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7fcac207fa40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 10] NCCL watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800933 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5cca99c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f5c6c758142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f5c6c75e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f5c6c75eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f5cca4b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f5cd9b0fac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f5cd9ba1a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f5cca99c8f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f5c6c786d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f5c6c4be879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f5cca4b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f5cd9b0fac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f5cd9ba1a40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 11] NCCL watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800939 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcab2e858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7fca54d58142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7fca54d5e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7fca54d5eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7fcab2ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fcac1fedac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7fcac207fa40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7fcab2e858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7fca54d86d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7fca54abe879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7fcab2ab0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7fcac1fedac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7fcac207fa40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank14]:[E ProcessGroupNCCL.cpp:768] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E ProcessGroupNCCL.cpp:774] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E ProcessGroupNCCL.cpp:1282] [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800941 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9cdc6858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f9c7e558142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f9c7e55e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f9c7e55eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f9cdc2b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f9ceb7e9ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f9ceb87ba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [Rank 14] NCCL watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2203, OpType=ALLREDUCE, NumelIn=6912000, NumelOut=6912000, Timeout(ms)=1800000) ran for 1800941 milliseconds before timing out.
Exception raised from checkTimeout at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:756 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9cdc6858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
srun: error: worker-0: tasks 8,12,14: Exited with exit code 134
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(c10::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1f2 (0x7f9c7e558142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x178 (0x7f9c7e55e538 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x8e (0x7f9c7e55eb2e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdc253 (0x7f9cdc2b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f9ceb7e9ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126a40 (0x7f9ceb87ba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1286 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7f9cdc6858f9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xf59d3e (0x7f9c7e586d3e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xc91879 (0x7f9c7e2be879 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xdc253 (0x7f9cdc2b0253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x94ac3 (0x7f9ceb7e9ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x126a40 (0x7f9ceb87ba40 in /usr/lib/x86_64-linux-gnu/libc.so.6)

./run_and_time.sh: line 53:  5954 Aborted                 python main.py lightning.trainer.num_nodes=${NUM_NODES} lightning.trainer.devices=${GPUS_PER_NODE} -m train --ckpt ${CHECKPOINT} --logdir ${RESULTS_DIR} -b ${CONFIG}
./run_and_time.sh: line 53:  5953 Aborted                 python main.py lightning.trainer.num_nodes=${NUM_NODES} lightning.trainer.devices=${GPUS_PER_NODE} -m train --ckpt ${CHECKPOINT} --logdir ${RESULTS_DIR} -b ${CONFIG}
./run_and_time.sh: line 53:  5955 Aborted                 python main.py lightning.trainer.num_nodes=${NUM_NODES} lightning.trainer.devices=${GPUS_PER_NODE} -m train --ckpt ${CHECKPOINT} --logdir ${RESULTS_DIR} -b ${CONFIG}
./run_and_time.sh: line 53:  5960 Aborted                 (core dumped) python main.py lightning.trainer.num_nodes=${NUM_NODES} lightning.trainer.devices=${GPUS_PER_NODE} -m train --ckpt ${CHECKPOINT} --logdir ${RESULTS_DIR} -b ${CONFIG}
srun: error: worker-0: task 15: Exited with exit code 134
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: got SIGCONT
slurmstepd: error: *** JOB 835 ON worker-1 CANCELLED AT 2024-06-15T20:49:40 ***
srun: forcing job termination
slurmstepd: error: *** STEP 835.0 ON worker-1 CANCELLED AT 2024-06-15T20:49:40 ***
srun: error: Timed out waiting for job step to complete
